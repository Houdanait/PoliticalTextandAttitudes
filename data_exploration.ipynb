{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matt\n",
      "   Total Transcripts: 1000\n",
      "   Fully Graded Transcripts: 102\n",
      "MW\n",
      "   Total Transcripts: 246\n",
      "   Fully Graded Transcripts: 98\n",
      "HNEB\n",
      "   Total Transcripts: 246\n",
      "   Fully Graded Transcripts: 246\n",
      "MW_merge\n",
      "   Total Transcripts: 203\n",
      "   Fully Graded Transcripts: 200\n"
     ]
    }
   ],
   "source": [
    "## How well do our datasets match?\n",
    "import json\n",
    "import re\n",
    "\n",
    "filenames = ['Matt', 'MW', 'HNEB', 'MW_merge']\n",
    "for filepath in filenames:\n",
    "    print(filepath)\n",
    "    with open(f\"data/filtered_utterances_sample_{filepath}.json\", 'r') as file:\n",
    "        transcripts = json.load(file)\n",
    "        print(\"   Total Transcripts:\",len(transcripts))\n",
    "        filtered_data = [item for item in transcripts if not any(value == \"ungraded\" for value in item[\"matched_terms\"].values())]\n",
    "        print(\"   Fully Graded Transcripts:\",len(filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching IDs (100)\n",
      "Unique in JSON1 (146)\n",
      "Unique in JSON2 (105)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_comparison = ['HNEB', 'MW_merge']\n",
    "\n",
    "with open(f\"data/filtered_utterances_sample_{file_comparison[0]}.json\", 'r') as file1:\n",
    "    json1 = json.load(file1)\n",
    "with open(f\"data/filtered_utterances_sample_{file_comparison[1]}.json\", 'r') as file2:\n",
    "    json2 = json.load(file2)\n",
    "    \n",
    "    \n",
    "set1 = set((item[\"transcript_id\"], item[\"statement\"]) for item in json1)\n",
    "set2 = set((item[\"transcript_id\"], item[\"statement\"]) for item in json2)\n",
    "\n",
    "# Finding matching and unique transcript_ids\n",
    "matching_ids = set1.intersection(set2)\n",
    "unique_in_json1 = set1.difference(set2)\n",
    "unique_in_json2 = set2.difference(set1)\n",
    "\n",
    "# Printing the results\n",
    "print(f\"Matching IDs ({len(matching_ids)})\")\n",
    "print(f\"Unique in JSON1 ({len(unique_in_json1)})\")\n",
    "print(f\"Unique in JSON2 ({len(unique_in_json2)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import group_by_key\n",
    "from collections import defaultdict\n",
    "# Grouping both JSON lists\n",
    "grouped_json1 = group_by_key(json1)\n",
    "grouped_json2 = group_by_key(json2)\n",
    "\n",
    "# Finding common keys\n",
    "common_keys = set(grouped_json1.keys()).intersection(grouped_json2.keys())\n",
    "\n",
    "# Merging matched terms for common keys\n",
    "merged_json = []\n",
    "for key in common_keys:\n",
    "    combined_matched_terms = defaultdict(list)\n",
    "    for item in grouped_json1[key] + grouped_json2[key]:\n",
    "        for term, value in item['matched_terms'].items():\n",
    "            if value not in combined_matched_terms[term]:\n",
    "                combined_matched_terms[term].append(value)\n",
    "                \n",
    "    \n",
    "    # Create a new entry for each common key with merged matched terms\n",
    "    new_entry = {\n",
    "        \"transcript_id\": key[0],\n",
    "        \"matched_terms\": dict(combined_matched_terms),\n",
    "        \"previous_statement\": grouped_json1[key][0][\"previous_statement\"],  # Example, using the first found\n",
    "        \"statement\": key[1]\n",
    "    }\n",
    "    merged_json.append(new_entry)\n",
    "\n",
    "# Print or process the resulting merged_json\n",
    "# print(merged_json)\n",
    "\n",
    "# Output the sampled data as a JSON file\n",
    "with open('data/filtered_utterances_samples_MW_HNEB_MERGE.json', 'w') as file:\n",
    "    json.dump(merged_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'given': 'none', 'could': 'hedge'}\n",
      "{}\n",
      "{'about': 'none'}\n",
      "{'all': 'none', 'every': 'none', 'quite': 'none', 'wonder': 'none', 'well': 'none'}\n",
      "{'think': 'hedge'}\n",
      "{'every': 'none', 'thing': 'none'}\n",
      "{'all': 'none', 'every': 'none', 'about': 'none', 'hedge': 'none', 'thing': 'none', 'sold': 'none', 'perspective': 'none'}\n",
      "{}\n",
      "{'all': 'none'}\n",
      "{'every': 'none', 'know': 'hedge', 'could': 'hedge', 'think': 'hedge', 'thing': 'none'}\n",
      "{'every': 'none'}\n",
      "{'sole': 'none'}\n",
      "{'all': 'none', 'well-known': 'none', 'allegedly': 'hedge', 'wrong': 'none', 'feeling': 'none', 'well': 'none'}\n",
      "{'take': 'none'}\n",
      "{'all': 'none', 'know': 'hedge', 'about': 'none', 'kind of': 'hedge', 'sort of': 'hedge', 'seem': 'hedge', 'question': 'none', 'thing': 'none', 'well': 'none'}\n",
      "{'all': 'none'}\n",
      "{}\n",
      "{}\n",
      "{'best': 'none', 'all': 'none', 'amend': 'none'}\n",
      "{'all': 'none', 'could': 'hedge', 'might': 'hedge', 'often': 'none', 'sort of': 'hedge', 'question': 'none', 'well': 'none'}\n",
      "{'only': 'none'}\n",
      "{'absolutely': 'authority'}\n",
      "{}\n",
      "{'best': 'none', 'every': 'none', 'know': 'none', 'only': 'none', 'probably': 'hedge', 'maybe': 'hedge', 'thing': 'none'}\n",
      "{'take': 'none'}\n",
      "{'most important': 'none', 'all': 'none'}\n",
      "{'all': 'none'}\n",
      "{'certainly': 'authority', 'obvious': 'none', 'only': 'none', 'about': 'none', 'think': 'hedge', 'kind of': 'hedge', 'thing': 'none'}\n",
      "{'might': 'hedge', 'reduce': 'none', 'perspective': 'none'}\n",
      "{}\n",
      "{'all': 'none', 'clear': 'none', 'may': 'hedge', 'question': 'none', 'thing': 'none'}\n",
      "{'all': 'none'}\n",
      "{'all': 'none', 'every': 'none'}\n",
      "{'all': 'none', 'every': 'none', 'quite': 'none', 'about': 'none', 'take': 'none'}\n",
      "{}\n",
      "{'all': 'none', 'change': 'none'}\n",
      "{'view': 'none', 'torn': 'none'}\n",
      "{'all': 'none'}\n",
      "{'all': 'none', 'about': 'none', 'question': 'none'}\n",
      "{'all': 'none', 'take': 'none'}\n",
      "{}\n",
      "{'may': 'hedge'}\n",
      "{'well': 'none'}\n",
      "{'clear': 'none', 'could': 'hedge', 'should': 'authority', 'about': 'none', 'think': 'hedge', 'question': 'none', 'change': 'none', 'adjust': 'none', 'well': 'none'}\n",
      "{'all': 'none', 'may': 'hedge', 'around': 'none', 'perhaps': 'hedge', 'take': 'none', 'appear': 'none', 'avoid': 'none', 'thing': 'none', 'sold': 'none'}\n",
      "{}\n",
      "{'sort of': 'hedge', 'take': 'none', 'alter': 'none', 'thing': 'none'}\n",
      "{'all': 'none', 'believe': 'hedge'}\n",
      "{'best': 'none', 'should': 'authority', 'about': 'none', 'take': 'none', 'view': 'none', 'question': 'none'}\n",
      "{}\n",
      "{'sometimes': 'hedge'}\n",
      "{'all': 'none', 'every': 'none', 'reality': 'none', 'about': 'none', 'reportedly': 'hedge', 'expect': 'none', 'question': 'none', 'thing': 'none', 'well': 'none'}\n",
      "{'truth': 'none', 'take': 'none'}\n",
      "{'all': 'none'}\n",
      "{'all': 'none', 'about': 'none', 'probably': 'hedge', 'view': 'none', 'appear': 'none', 'thing': 'none'}\n",
      "{'all': 'none', 'sort of': 'hedge', 'thing': 'none'}\n",
      "{'sometimes': 'hedge', 'maybe': 'hedge', 'moderate': 'none', 'thing': 'none', 'well': 'none'}\n",
      "{'take': 'none'}\n",
      "{}\n",
      "{'all': 'none'}\n",
      "{'change': 'none'}\n",
      "{'all': 'none', 'might': 'hedge', 'about': 'none', 'kind of': 'hedge', 'thing': 'none'}\n",
      "{'all': 'none'}\n",
      "{'all': 'none', 'every': 'none', 'need to': 'authority', 'have to': 'authority', 'know': 'none', 'sure': 'none', 'about': 'none', 'around': 'none', 'thing': 'none'}\n",
      "{'completely': 'none'}\n",
      "{'every': 'none', 'thing': 'none'}\n",
      "{'all': 'none', 'about': 'none', 'thing': 'none'}\n",
      "{'totally': 'none', 'would': 'hedge', 'about': 'none', 'kind of': 'hedge', 'tend': 'hedge', 'question': 'none', 'avoid': 'none'}\n",
      "{'maybe': 'hedge', 'sold': 'none'}\n",
      "{'about': 'none'}\n",
      "{}\n",
      "{'take': 'none', 'well': 'none'}\n",
      "{'sure': 'hedge', 'well': 'none'}\n",
      "{'might': 'hedge', 'take': 'none'}\n",
      "{'all': 'none', 'always': 'none', 'final word': 'none', 'quite': 'none', 'perhaps': 'hedge', 'kind of': 'hedge', 'well': 'none'}\n",
      "{'sometimes': 'hedge', 'about': 'none', 'reportedly': 'hedge', 'thing': 'none'}\n",
      "{'all': 'none', 'well': 'none'}\n",
      "{'all': 'none', 'about': 'none'}\n",
      "{'all': 'none', 'wrong': 'none', 'take': 'none'}\n",
      "{'all': 'none', 'clear': 'none', 'question': 'none'}\n",
      "{'all': 'none', 'every': 'none', 'need to': 'authority', 'think': 'hedge', 'a little': 'hedge', 'hope': 'hedge'}\n",
      "{}\n",
      "{'perspective': 'none'}\n",
      "{'obvious': 'none', 'about': 'none'}\n",
      "{}\n",
      "{'every': 'none'}\n",
      "{}\n",
      "{}\n",
      "{'all': 'none', 'tend': 'none'}\n",
      "{'all': 'none', 'clear': 'none', 'around': 'none', 'view': 'none', 'thing': 'none'}\n",
      "{'all': 'none', 'about': 'none', 'think': 'hedge'}\n",
      "{'somewhat': 'hedge'}\n",
      "{'all': 'none', 'every': 'none', 'obvious': 'none', 'essential': 'none', 'apparent': 'none', 'may': 'hedge', 'might': 'hedge', 'about': 'none', 'around': 'none', 'maybe': 'hedge', 'think': 'hedge', 'kind of': 'hedge', 'sort of': 'hedge', 'seem': 'hedge', 'posit': 'none', 'change': 'none', 'temper': 'none', 'certain': 'none', 'positive': 'none'}\n",
      "{'all': 'none', 'every': 'none'}\n",
      "{'all': 'none', 'thing': 'none', 'well': 'none'}\n",
      "{'only': 'none', 'imagine': 'none', 'well': 'none'}\n",
      "{'every': 'none', 'about': 'none', 'tend': 'none'}\n",
      "{'never': 'hedge', 'seem': 'hedge'}\n",
      "{'all': 'none', 'believe': 'none'}\n",
      "{'well': 'none'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "# Load the JSON data from a file\n",
    "with open(\"data/filtered_utterances_samples_MW_HNEB_MERGE.json\", 'r') as file1:\n",
    "    data = json.load(file1)  # Changed variable name from 'json' to 'data'\n",
    "final_list = []\n",
    "# Iterate through each item in the data list\n",
    "for item in data:\n",
    "    # Print each term in the 'matched_terms' dictionary\n",
    "    PrMs = {}\n",
    "    for key, value in item['matched_terms'].items():\n",
    "        if len(value) == 1:\n",
    "        #    print(key, value)\n",
    "           PrMs[key] = value[0]\n",
    "    print(PrMs)\n",
    "    if len(PrMs)>0:\n",
    "        sample = {\n",
    "            \"transcript_id\": item['transcript_id'],\n",
    "            \"statement_id\": str(uuid.uuid4()),  # Generate a random UUID for the 'statement_id\n",
    "            \"matched_terms\": PrMs,\n",
    "            \"previous_statement\": item['previous_statement'],\n",
    "            \"statement\": item['statement']\n",
    "        }\n",
    "        # print(sample)\n",
    "        final_list.append(sample)\n",
    "\n",
    "# Convert all sets to lists in the final_list\n",
    "def convert_sets_to_lists(item):\n",
    "    if isinstance(item, set):\n",
    "        return list(item)\n",
    "    elif isinstance(item, dict):\n",
    "        return {key: convert_sets_to_lists(value) for key, value in item.items()}\n",
    "    elif isinstance(item, list):\n",
    "        return [convert_sets_to_lists(element) for element in item]\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "final_list_converted = [convert_sets_to_lists(item) for item in final_list]\n",
    "\n",
    "with open('data/filtered_utterances_ft_data.json', 'w') as file:\n",
    "    json.dump(final_list_converted, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'all': ['n', 'o', 'e']})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(combined_matched_terms)\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Check if any term has exactly one matched term\u001b[39;00m\n\u001b[1;32m     27\u001b[0m has_single_match_terms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m values \u001b[38;5;129;01min\u001b[39;00m combined_matched_terms\u001b[38;5;241m.\u001b[39mvalues())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from util import group_by_key\n",
    "from collections import defaultdict\n",
    "# Grouping both JSON lists\n",
    "grouped_json1 = group_by_key(json1)\n",
    "grouped_json2 = group_by_key(json2)\n",
    "\n",
    "# Finding common keys\n",
    "common_keys = set(grouped_json1.keys()).intersection(grouped_json2.keys())\n",
    "\n",
    "# Merging matched terms for common keys\n",
    "merged_json = []\n",
    "for key in common_keys:\n",
    "    combined_matched_terms = defaultdict(list)\n",
    "    for item in grouped_json1[key] + grouped_json2[key]:\n",
    "        for term, value in item['matched_terms'].items():\n",
    "            if value not in combined_matched_terms[term]:\n",
    "                combined_matched_terms[term].append(value)\n",
    "    \n",
    "    \n",
    "    # Create a new entry for each common key with merged matched terms\n",
    "    new_entry = {\n",
    "        \"transcript_id\": key[0],\n",
    "        \"matched_terms\": dict(combined_matched_terms),\n",
    "        \"previous_statement\": grouped_json1[key][0][\"previous_statement\"],  # Example, using the first found\n",
    "        \"statement\": key[1]\n",
    "    }\n",
    "    merged_json.append(new_entry)\n",
    "\n",
    "# Print or process the resulting merged_json\n",
    "# print(merged_json)\n",
    "\n",
    "# Output the sampled data as a JSON file\n",
    "with open('data/filtered_utterances_samples_MW_HNEB_MERGE.json', 'w') as file:\n",
    "    json.dump(merged_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:       100\n",
      "Total number of matched terms: 414\n",
      "Terms per sample utterance:    4.14\n",
      "Perfect Match Terms:           66.4%  (275/414)\n",
      "PrM Positive Match Count:           65\n",
      "Total Match Statements:        34.0%  ( 34/ 100)\n"
     ]
    }
   ],
   "source": [
    "json_data = merged_json\n",
    "# Initialize counters\n",
    "total_samples = len(json_data)\n",
    "total_matched_terms = 0\n",
    "total_length_one = 0\n",
    "total_length_more_than_one = 0\n",
    "all_length_one = 0\n",
    "any_length_more_than_one = 0\n",
    "actual_prm = 0\n",
    "\n",
    "# Analysis of each sample\n",
    "for sample in json_data:\n",
    "    matched_terms = sample['matched_terms']\n",
    "    sample_terms_count = len(matched_terms)\n",
    "    total_matched_terms += sample_terms_count\n",
    "\n",
    "    length_one_count = 0\n",
    "    length_more_than_one_count = 0\n",
    "\n",
    "    for term, values in matched_terms.items():\n",
    "        if len(values) == 1:\n",
    "            total_length_one += 1\n",
    "            length_one_count += 1\n",
    "            if values[0] == \"hedge\":\n",
    "                actual_prm+=1\n",
    "            if values[0] == \"authority\":\n",
    "                actual_prm+=1\n",
    "        elif len(values) > 1:\n",
    "            total_length_more_than_one += 1\n",
    "            length_more_than_one_count += 1\n",
    "\n",
    "    # Check if all terms in a sample have lists of length 1\n",
    "    if length_one_count == sample_terms_count:\n",
    "        all_length_one += 1\n",
    "    \n",
    "    # Check if any term in a sample has a list longer than 1\n",
    "    if length_more_than_one_count > 0:\n",
    "        any_length_more_than_one += 1\n",
    "\n",
    "# Printing the results\n",
    "print(\"Total number of samples:      \", total_samples)\n",
    "print(\"Total number of matched terms:\", total_matched_terms)\n",
    "print(f\"Terms per sample utterance:    {total_matched_terms / total_samples:.2f}\")\n",
    "print(f\"Perfect Match Terms:           {round(100*total_length_one / total_matched_terms, 1)}%  ({total_length_one}/{total_matched_terms})\")\n",
    "print(f\"PrM Positive Match Count:           {actual_prm}\")\n",
    "print(f\"Total Match Statements:        {round(100*all_length_one / total_samples, 1)}%  ( {all_length_one}/ {total_samples})\")\n",
    "# print(\"Total samples where at least one matched term has list length > 1:\", any_length_more_than_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and: 462792\n",
      "but: 447460\n",
      "so: 419813\n",
      "now: 395928\n",
      "or: 391821\n",
      "like: 360746\n",
      "also: 340402\n",
      "really: 322127\n",
      "even: 297664\n",
      "first: 296996\n",
      "then: 287166\n",
      "look: 267461\n",
      "actually: 238482\n",
      "next: 217319\n",
      "well: 194280\n",
      "listen: 83200\n",
      "finally: 54925\n",
      "totally: 35285\n",
      "yes: 22851\n",
      "oh: 14775\n",
      "besides: 8540\n",
      "un: 4137\n",
      "genuinely: 3093\n",
      "okay: 2047\n",
      "hence: 1121\n",
      "admittedly: 862\n",
      "likewise: 662\n",
      "er: 498\n",
      "uh: 272\n",
      "alright: 161\n",
      "cr: 150\n",
      "anyhow: 113\n",
      "alternatively: 100\n",
      "conversely: 95\n",
      "em: 59\n",
      "lol: 56\n",
      "uhu: 1\n"
     ]
    }
   ],
   "source": [
    "from util import count_prms_in_utt, print_sorted_json\n",
    "# Filepaths for the JSON files\n",
    "prms_filepath = 'data/prms.json'\n",
    "large_json_filepath = 'data/news_dialogue_sample.json'\n",
    "output_filepath = 'data/prms_sample_count.json'\n",
    "\n",
    "# Call the function on sample JSON\n",
    "count_prms_in_utt(prms_filepath, large_json_filepath, output_filepath)\n",
    "\n",
    "# Call the function on full JSON\n",
    "# large_json_filepath = 'data/news_dialogue.json'\n",
    "# output_filepath = 'data/prms_full_count.json'\n",
    "# count_prms_in_utt(prms_filepath, large_json_filepath, output_filepath)\n",
    "\n",
    "# Path to the JSON file\n",
    "input_filepath = 'data/prms_full_count.json'\n",
    "# Call the function to print sorted JSON\n",
    "print_sorted_json(input_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import print_sample_json\n",
    "# \n",
    "# print_sample_json('data/news_dialogue_sample.json', sample_size=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
