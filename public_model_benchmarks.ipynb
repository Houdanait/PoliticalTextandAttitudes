{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from pyauth import openaikey\n",
    "\n",
    "client = OpenAI(api_key=openaikey)\n",
    "\n",
    "sample_text = {\n",
    "        \"transcript_id\": \"CNN-235715\",\n",
    "        \"statement_id\": \"640bf44d-6500-4a11-a473-4195d48baf31\",\n",
    "        \"matched_terms\": {\n",
    "            \"about\": \"none\",\n",
    "            \"know\": \"none\"\n",
    "        },\n",
    "        \"previous_statement\": \"Actually this is\",\n",
    "        \"statement\": \"The Vines that you've posted show some incredible sights and sounds, rockets, shaking buildings, ambulances in the night. What do you want people to <KNOW> <ABOUT> life inside Gaza?\"\n",
    "    }\n",
    "\n",
    "# transcript_id, statement_id, matched_terms, matched_terms_list, previous_statement, statement, matched_terms_list, string = parse_json(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # print(system_prompt)\n",
    "# # # print(string)\n",
    "\n",
    "# google_prompt = system_prompt + \"\\n-----BEGIN INPUT-----\\n\" + string\n",
    "# print(google_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwise/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from pyauth import gemini_api_key\n",
    "genai.configure(api_key= gemini_api_key)\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash',\n",
    "                              generation_config={\"response_mime_type\": \"application/json\"})\n",
    "\n",
    "# prompt = google_prompt\n",
    "\n",
    "# response = model.generate_content(prompt)\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='terms: [\\'about\\', \\'know\\']\\ntranscript:\\nSpeaker 1: \"Actually this is\"\\nSpeaker 2: \"The Vines that you\\'ve posted show some incredible sights and sounds, rockets, shaking buildings, ambulances in the', type='text')]\n"
     ]
    }
   ],
   "source": [
    "# import anthropic\n",
    "# from pyauth import claudeapikey\n",
    "\n",
    "# client = anthropic.Client(api_key=claudeapikey)\n",
    "\n",
    "# response = client.messages.create(\n",
    "#     model=\"claude-2.1\",\n",
    "#     max_tokens=50,\n",
    "#     system=system_prompt, # <-- system prompt\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": string} # <-- user prompt\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_01RoFtC87msrxzRQYrxxaB8j', content=[TextBlock(text=\"Unfortunately there are no terms provided in brackets in the transcript. The terms list contains 'about' and 'know', but those words do not appear bracketed in capital letters in the transcript. So there are no terms I can analyze and provide answers for. Please provide a transcript with bracketed and capitalized terms that match the list of terms you want me to analyze.\", type='text')], model='claude-2.1', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=482, output_tokens=78))\n"
     ]
    }
   ],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def parse_json(json_response, verbose=False):\n",
    "    matched_terms_list = []\n",
    "    for matched_term in json_response[\"matched_terms\"]:\n",
    "        matched_terms_list.append(matched_term)\n",
    "\n",
    "    string = f\"\"\"terms: {matched_terms_list}\"\\nTranscript:\\nSpeaker 1: \"{json_response[\"previous_statement\"]}\"\\nSpeaker 2: \"{json_response[\"statement\"]}\" \"\"\"\n",
    "    \n",
    "    transcript_id = json_response[\"transcript_id\"]\n",
    "    statement_id = json_response[\"statement_id\"]\n",
    "    if verbose:\n",
    "        print(\"Parsing\", statement_id)\n",
    "    previous_statement = json_response[\"previous_statement\"]\n",
    "    statement = json_response[\"statement\"]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Parsed JSON:\", transcript_id, statement_id, matched_terms_list, previous_statement, statement)\n",
    "    matched_terms = json_response[\"matched_terms\"]\n",
    "\n",
    "    return transcript_id, statement_id, matched_terms, matched_terms_list, previous_statement, statement, matched_terms_list, string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_json_response_gemini(string, model_name='gemini-1.5-flash', system_prompt=system_prompt, verbose=False):\n",
    "    model = genai.GenerativeModel(model_name,\n",
    "                              generation_config={\"response_mime_type\": \"application/json\"})\n",
    "\n",
    "    prompt = system_prompt + \"\\n-----BEGIN INPUT-----\\n\" + string\n",
    "\n",
    "    response = model.generate_content(prompt).text\n",
    "    # print(response)\n",
    "    try:\n",
    "        parsed_response = json.loads(response)\n",
    "        if verbose:\n",
    "            print(\"Received response from model.\")\n",
    "\n",
    "        return parsed_response\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON response.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        time.sleep(60)\n",
    "\n",
    "def get_json_response(string, model=\"gpt-3.5-turbo\", system_prompt=system_prompt, verbose=False):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": string}\n",
    "        ]\n",
    "    )\n",
    "    completion = response.choices[0].message.content\n",
    "    try:\n",
    "        parsed_response = json.loads(completion)\n",
    "        if verbose:\n",
    "            print(\"Received response from model.\")\n",
    "        return parsed_response\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON response.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "def update_grade_json_gpt(json_example, model = \"gpt-3.5-turbo\", verbose=False):\n",
    "    transcript_id, statement_id, matched_terms, matched_terms_list, previous_statement, statement, matched_terms_list, string = parse_json(json_example)\n",
    "    parsed_response = get_json_response(string)\n",
    "    output_matched_items = {}\n",
    "    # print(\"Matched Terms:\",matched_terms_list)\n",
    "    for item in matched_terms_list:\n",
    "        if verbose:\n",
    "            print(\"Matching item:\", item)\n",
    "            print(\"   Correct Response:\", matched_terms[item])\n",
    "            print(\"   Model Response:\", parsed_response[item])\n",
    "        item_vals = {\n",
    "            \"correct\": matched_terms[item],\n",
    "            model : parsed_response[item]\n",
    "        }\n",
    "        output_matched_items[item] = item_vals\n",
    "\n",
    "    benchmark_output = {\n",
    "        \"transcript_id\": transcript_id,\n",
    "        \"statement_id\": statement_id,\n",
    "        \"matched_terms\": output_matched_items,\n",
    "        \"previous_statement\": previous_statement,\n",
    "        \"statement\": statement\n",
    "    }\n",
    "    if verbose:\n",
    "        print(\"Completed grading:\", benchmark_output)    \n",
    "    return benchmark_output\n",
    "\n",
    "def insert_new_row_json(json_example, model = \"gpt-3.5-turbo\", verbose=False):\n",
    "    transcript_id, statement_id, matched_terms, matched_terms_list, previous_statement, statement, matched_terms_list, string = parse_json(json_example)\n",
    "    parsed_response = get_json_response(string)\n",
    "    output_matched_items = {}\n",
    "    # print(\"Matched Terms:\",matched_terms_list)\n",
    "    for item in matched_terms_list:\n",
    "        if verbose:\n",
    "            print(\"Matching item:\", item)\n",
    "            print(\"   Correct Response:\", matched_terms[item])\n",
    "            print(\"   Model Response:\", parsed_response[item])\n",
    "        item_vals = {\n",
    "            \"correct\": matched_terms[item],\n",
    "            model : parsed_response[item]\n",
    "        }\n",
    "        output_matched_items[item] = item_vals\n",
    "\n",
    "    benchmark_output = {\n",
    "        \"transcript_id\": transcript_id,\n",
    "        \"statement_id\": statement_id,\n",
    "        \"matched_terms\": output_matched_items,\n",
    "        \"previous_statement\": previous_statement,\n",
    "        \"statement\": statement\n",
    "    }\n",
    "    if verbose:\n",
    "        print(\"Completed grading:\", benchmark_output)    \n",
    "    return benchmark_output\n",
    "\n",
    "\n",
    "# x = update_grade_json_gpt(text)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get GPT3.5 Responses\n",
    "\n",
    "Update code to also incorporate 4, gemini, etc. Need to have the code update if something already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [\"correct\", \"gpt-3.5-turbo\", \"gpt-4o\", \"gpt-4-turbo\", \"gemini-1.5-flash\", \"gemini-1.5-pro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 741 examples from data/human_annotated_model_response.json\n",
      "Processed and saved 100 samples, with 0 Gemini samples\n",
      "Processed and saved 200 samples, with 0 Gemini samples\n",
      "Processed and saved 300 samples, with 0 Gemini samples\n",
      "Processed and saved 400 samples, with 0 Gemini samples\n",
      "Processed and saved 500 samples, with 0 Gemini samples\n",
      "Processed and saved 600 samples, with 2 Gemini samples\n",
      "Processed and saved 606 samples, with 14 Gemini samples\n",
      "Processed and saved 621 samples, with 44 Gemini samples\n",
      "Processed and saved 636 samples, with 74 Gemini samples\n",
      "Processed and saved 651 samples, with 104 Gemini samples\n",
      "Processed and saved 666 samples, with 134 Gemini samples\n",
      "Processed and saved 681 samples, with 164 Gemini samples\n",
      "Processed and saved 696 samples, with 194 Gemini samples\n",
      "Processed and saved 700 samples, with 202 Gemini samples\n",
      "Processed and saved 711 samples, with 224 Gemini samples\n",
      "Processed and saved 726 samples, with 254 Gemini samples\n",
      "Processed and saved 741 samples, with 284 Gemini samples\n",
      "\n",
      "Model Tally:\n",
      " correct: 741\n",
      " gpt-3.5-turbo: 741\n",
      " gpt-4o: 741\n",
      " gpt-4-turbo: 741\n",
      " gemini-1.5-flash: 741\n",
      " gemini-1.5-pro: 741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Load the JSON data from a file\n",
    "file_path = 'data/human_annotated_model_response.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    ground_truth = json.load(file)\n",
    "\n",
    "print(\"Loaded\", len(ground_truth), \"examples from\", file_path)\n",
    "\n",
    "# output_dict = {f\"{{sample['statement_id']}\": sample for sample in ground_truth}\n",
    "output_dict = {f\"{sample['transcript_id']}_{sample['statement_id']}\": sample for sample in ground_truth}\n",
    "\n",
    "tally = {model: 0 for model in models_list}\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for sample in ground_truth:  # Adjust this to process more samples if needed\n",
    "    j+=1\n",
    "    transcript_id, statement_id, matched_terms, matched_terms_list, previous_statement, statement, matched_terms_list, string = parse_json(sample)\n",
    "    \n",
    "    # print(\"Matched_terms\", matched_terms)\n",
    "\n",
    "    # Iterate through only the first matched term\n",
    "    first_term, models = next(iter(matched_terms.items()))\n",
    "    \n",
    "    # Iterate through each model name and its matched value\n",
    "    for model_name in models_list:\n",
    "        if model_name in models:\n",
    "            # print(\"Skipping model\", model_name)\n",
    "            tally[model_name] += 1\n",
    "        else:\n",
    "            # response = get_json_response(string, model=model_name)\n",
    "            if model_name == \"correct\":\n",
    "                asdfas = 0\n",
    "            elif \"gpt\" in model_name:\n",
    "                try:\n",
    "                    response = get_json_response(string, model=model_name)\n",
    "                    # print(response)\n",
    "                    for item in matched_terms:\n",
    "                        matched_terms[item][model_name] = response[item]\n",
    "                    tally[model_name] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error on {model_name}: {e}\")\n",
    "            elif \"gemini\" in model_name:\n",
    "                try:\n",
    "                    response = get_json_response_gemini(string, model_name=model_name)\n",
    "                    for item in matched_terms:\n",
    "                        matched_terms[item][model_name] = response[item]\n",
    "                    tally[model_name] += 1\n",
    "                    i+=1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error on {model_name}: {e}\")\n",
    "                    # print(f\"Error on {model_name}: {e}, on string: {string}\")\n",
    "            else:\n",
    "                \"Model Not Recognized\"\n",
    "\n",
    "    sample_output = {\n",
    "        \"transcript_id\": transcript_id,\n",
    "        \"statement_id\": statement_id,\n",
    "        \"matched_terms\": matched_terms,\n",
    "        \"previous_statement\": previous_statement,\n",
    "        \"statement\": statement\n",
    "    }\n",
    "    output_dict[f\"{transcript_id}_{statement_id}\"] = sample_output\n",
    "    if (i+1) % 15 == 0:\n",
    "        output = list(output_dict.values())\n",
    "\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(output, file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Processed and saved {j} samples, with {i} Gemini samples\")\n",
    "        # time.sleep(60)\n",
    "    elif j % 100 == 0:\n",
    "        output = list(output_dict.values())\n",
    "\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(output, file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Processed and saved {j} samples, with {i} Gemini samples\")\n",
    "\n",
    "output = list(output_dict.values())\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(output, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print the tally\n",
    "print(\"\\nModel Tally:\")\n",
    "for model, count in tally.items():\n",
    "    print(f\" {model}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(file_path, 'w') as file:\n",
    "#     json.dump(model_responses, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Results for Model: correct\n",
      " * Class: none\n",
      "   F1 Score: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   Accuracy: 1.0000\n",
      "\n",
      " * Class: hedge\n",
      "   F1 Score: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   Accuracy: 1.0000\n",
      "\n",
      " * Class: authority\n",
      "   F1 Score: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   Accuracy: 1.0000\n",
      "\n",
      "Overall Model Performance:\n",
      "Average F1 Score: 1.0000\n",
      "Average Precision: 1.0000\n",
      "Average Recall: 1.0000\n",
      "Total Accuracy: 1.0000\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Results for Model: gpt-3.5-turbo\n",
      " * Class: none\n",
      "   F1 Score: 0.5026\n",
      "   Precision: 0.3462\n",
      "   Recall: 0.9170\n",
      "   Accuracy: 0.3356\n",
      "\n",
      " * Class: hedge\n",
      "   F1 Score: 0.6774\n",
      "   Precision: 0.7664\n",
      "   Recall: 0.6068\n",
      "   Accuracy: 0.5121\n",
      "\n",
      " * Class: authority\n",
      "   F1 Score: 0.4880\n",
      "   Precision: 0.8918\n",
      "   Recall: 0.3359\n",
      "   Accuracy: 0.3228\n",
      "\n",
      "Overall Model Performance:\n",
      "Average F1 Score: 0.5560\n",
      "Average Precision: 0.6681\n",
      "Average Recall: 0.6199\n",
      "Total Accuracy: 0.5639\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Results for Model: gpt-4o\n",
      " * Class: none\n",
      "   F1 Score: 0.6034\n",
      "   Precision: 0.4345\n",
      "   Recall: 0.9871\n",
      "   Accuracy: 0.4320\n",
      "\n",
      " * Class: hedge\n",
      "   F1 Score: 0.7628\n",
      "   Precision: 0.9660\n",
      "   Recall: 0.6302\n",
      "   Accuracy: 0.6165\n",
      "\n",
      " * Class: authority\n",
      "   F1 Score: 0.6703\n",
      "   Precision: 0.9433\n",
      "   Recall: 0.5199\n",
      "   Accuracy: 0.5041\n",
      "\n",
      "Overall Model Performance:\n",
      "Average F1 Score: 0.6788\n",
      "Average Precision: 0.7813\n",
      "Average Recall: 0.7124\n",
      "Total Accuracy: 0.6836\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Results for Model: gpt-4-turbo\n",
      " * Class: none\n",
      "   F1 Score: 0.7357\n",
      "   Precision: 0.6026\n",
      "   Recall: 0.9442\n",
      "   Accuracy: 0.5818\n",
      "\n",
      " * Class: hedge\n",
      "   F1 Score: 0.7826\n",
      "   Precision: 0.9592\n",
      "   Recall: 0.6609\n",
      "   Accuracy: 0.6429\n",
      "\n",
      " * Class: authority\n",
      "   F1 Score: 0.7404\n",
      "   Precision: 0.8454\n",
      "   Recall: 0.6586\n",
      "   Accuracy: 0.5878\n",
      "\n",
      "Overall Model Performance:\n",
      "Average F1 Score: 0.7529\n",
      "Average Precision: 0.8024\n",
      "Average Recall: 0.7546\n",
      "Total Accuracy: 0.7554\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Results for Model: gemini-1.5-flash\n",
      " * Class: none\n",
      "   F1 Score: 0.5464\n",
      "   Precision: 0.3818\n",
      "   Recall: 0.9606\n",
      "   Accuracy: 0.3759\n",
      "\n",
      " * Class: hedge\n",
      "   F1 Score: 0.7725\n",
      "   Precision: 0.9433\n",
      "   Recall: 0.6541\n",
      "   Accuracy: 0.6293\n",
      "\n",
      " * Class: authority\n",
      "   F1 Score: 0.5877\n",
      "   Precision: 0.9330\n",
      "   Recall: 0.4289\n",
      "   Accuracy: 0.4161\n",
      "\n",
      "Overall Model Performance:\n",
      "Average F1 Score: 0.6355\n",
      "Average Precision: 0.7527\n",
      "Average Recall: 0.6812\n",
      "Total Accuracy: 0.6470\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Results for Model: gemini-1.5-pro\n",
      " * Class: none\n",
      "   F1 Score: 0.6465\n",
      "   Precision: 0.4872\n",
      "   Recall: 0.9607\n",
      "   Accuracy: 0.4777\n",
      "\n",
      " * Class: hedge\n",
      "   F1 Score: 0.7870\n",
      "   Precision: 0.9637\n",
      "   Recall: 0.6651\n",
      "   Accuracy: 0.6489\n",
      "\n",
      " * Class: authority\n",
      "   F1 Score: 0.6530\n",
      "   Precision: 0.9021\n",
      "   Recall: 0.5117\n",
      "   Accuracy: 0.4848\n",
      "\n",
      "Overall Model Performance:\n",
      "Average F1 Score: 0.6955\n",
      "Average Precision: 0.7843\n",
      "Average Recall: 0.7125\n",
      "Total Accuracy: 0.7046\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from a file\n",
    "file_path = 'data/human_annotated_model_response.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data_list = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to hold the confusion matrix\n",
    "\n",
    "for model in models_list:\n",
    "    # Populate confusion matrix from the JSON data\n",
    "    confusion_matrix = {}\n",
    "    for entry in data_list:\n",
    "        matched_terms = entry[\"matched_terms\"]\n",
    "        for term, details in matched_terms.items():\n",
    "            correct_answer = details[\"correct\"]\n",
    "            model_answer = details[model]\n",
    "\n",
    "            if correct_answer not in confusion_matrix:\n",
    "                confusion_matrix[correct_answer] = {}\n",
    "\n",
    "            if model_answer not in confusion_matrix[correct_answer]:\n",
    "                confusion_matrix[correct_answer][model_answer] = 0\n",
    "\n",
    "            confusion_matrix[correct_answer][model_answer] += 1\n",
    "\n",
    "    # Calculate precision, recall, F1, and accuracy for each class\n",
    "    print(\"###\"*20)\n",
    "\n",
    "    print(\"Results for Model:\", model)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    accuracies = []\n",
    "\n",
    "    total_correct = 0\n",
    "    total_instances = 0\n",
    "\n",
    "    for correct_class, predictions in confusion_matrix.items():\n",
    "        true_positive = predictions.get(correct_class, 0)\n",
    "        false_positive = sum(predictions.get(pred, 0) for pred in predictions if pred != correct_class)\n",
    "        false_negative = sum(confusion_matrix.get(pred, {}).get(correct_class, 0) for pred in confusion_matrix if pred != correct_class)\n",
    "        true_negative = total_instances - (true_positive + false_positive + false_negative)  # Not used directly\n",
    "\n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = true_positive / (true_positive + false_positive + false_negative) if (true_positive + false_positive + false_negative) > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        total_correct += true_positive\n",
    "        total_instances += sum(predictions.values())\n",
    "\n",
    "        print(f\" * Class: {correct_class}\")\n",
    "        print(f\"   F1 Score: {f1:.4f}\")\n",
    "        print(f\"   Precision: {precision:.4f}\")\n",
    "        print(f\"   Recall: {recall:.4f}\")\n",
    "        print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "        print()\n",
    "\n",
    "    print(\"Overall Model Performance:\")\n",
    "    print(f\"Average F1 Score: {sum(f1_scores) / len(f1_scores):.4f}\")\n",
    "    print(f\"Average Precision: {sum(precisions) / len(precisions):.4f}\")\n",
    "    print(f\"Average Recall: {sum(recalls) / len(recalls):.4f}\")\n",
    "    print(f\"Total Accuracy: {total_correct / total_instances:.4f}\")\n",
    "    print(\"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
