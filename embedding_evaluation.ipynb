{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: know\n",
      "  hedge: 2\n",
      "  none: 2\n",
      "Term: believe\n",
      "  hedge: 1\n",
      "  none: 1\n",
      "Term: sure\n",
      "  none: 1\n",
      "  hedge: 1\n",
      "Term: tend\n",
      "  hedge: 1\n",
      "  none: 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Load your JSON data\n",
    "with open('data/filtered_utterances_ft_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to hold the count of each value for each term\n",
    "term_counts = {}\n",
    "\n",
    "# Iterate through each item in the JSON data\n",
    "for item in data:\n",
    "    matched_terms = item['matched_terms']\n",
    "    for term, value in matched_terms.items():\n",
    "        if term not in term_counts:\n",
    "            term_counts[term] = Counter()\n",
    "        term_counts[term][value] += 1\n",
    "\n",
    "# Output the results\n",
    "for term, counts in term_counts.items():\n",
    "    if len(counts)>1:\n",
    "        print(f\"Term: {term}\")\n",
    "        for value, count in counts.items():\n",
    "            print(f\"  {value}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load your JSON data\n",
    "with open('data/filtered_utterances_ft_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "word = \"know\"\n",
    "# Filter data to include entries where 'matched_terms' has 'know' as a key\n",
    "filtered_data = [item for item in data if word in item['matched_terms']]\n",
    "statements = []\n",
    "# Optionally, print the filtered data to see the result\n",
    "for item in filtered_data:\n",
    "    # Concatenate strings properly\n",
    "    string = item[\"previous_statement\"] + \" statement: \" + item[\"statement\"]\n",
    "    category = item[\"matched_terms\"][\"know\"]\n",
    "    # Remove '<' and '>' characters from the string\n",
    "    string = string.replace(\"<\", \"\")\n",
    "    string = string.replace(\">\", \"\")\n",
    "    statements.append((string, category, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "sample_text = \"\"\" \"How do you suppose the royals even feel about this?\",\n",
    "        \"statement\": \"WELL, one THING ABOUT -- WELL, I mean, William and Harry are OBVIOUSLY our main concern. The NLY NG William has really said   this is that when he reads  his mother, it's very rarely the person he actually knew. CERTAINLY the person who wrote this article is Tina Brown, who has made something of a career making a career out of claiming to be one of Diana's closest friends. Reading the article, I don't <<THINK>> Tina Brown knew Diana at ALL>. There's a hell of a lot ABOUT it that's just WRONG>. The <KIND OF> political aspects of it where Diana WOULD be politically, COMPLETELY WRONG>. It's somebody I don't <<THINK>> knew Diana at ALL>.\" \"\"\"\n",
    "\n",
    "encoded_input = tokenizer(sample_text, return_tensors = 'pt')\n",
    "\n",
    "output = model(**encoded_input)\n",
    "\n",
    "# print(output[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know 1,2: hedge/hedge Cosine similarity: 0.8926\n",
      "know 1,3: hedge/none Cosine similarity: 0.7740\n",
      "know 1,4: hedge/none Cosine similarity: 0.3465\n",
      "know 2,3: hedge/none Cosine similarity: 0.8332\n",
      "know 2,4: hedge/none Cosine similarity: 0.4173\n",
      "know 3,4: none/none Cosine similarity: 0.3577\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the pretrained BERT base uncased model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Sample text\n",
    "# sample_texts = [\"\"\" \"How do you suppose the royals even feel about this?\",\n",
    "#         \"statement\": \"WELL, one THING ABOUT -- WELL, I mean, William and Harry are OBVIOUSLY our main concern. The ONLY THING William has really said about this is that when he reads about his mother, it's very rarely the person he actually knew. CERTAINLY the person who wrote this article is Tina Brown, who has made something of a career making a career out of claiming to be one of Diana's closest friends. Reading the article, I don't THINK Tina Brown knew Diana at ALL. There's a hell of a lot about it that's just WRONG. The KIND OF political aspects of it where Diana WOULD be politically, COMPLETELY WRONG. It's somebody I don't THINK knew Diana at ALL.\" \"\"\",        \n",
    "#             \"\"\" \"The reactions show that we have not come back to full local balance because when you're talking about the sustainable growth in all world regions, and I would also mean that any announcements of central bankers should not cause this dramatic fluctuations and should not actually cause those price changes. So Italy tells you how much that global economy depends on the action of central banks. And otherwise a calm and sustainable situation that obviously would not happen to that extent. So that shouldn't stop us to return back to the normal and sustainable path of development.\",\n",
    "#         \"statement\": \"So, Richard, very <CLEAR> here that both Chancellor Merkel and President Putin support this move of trying to start making that transition out from the very generous money. He was speaking <ABOUT> Greece today and the selloff that we had. I posed a <QUESTION> to Ms. Merkel as <WELL> <ABOUT> whether it's time to <CHANGE> the policy of austerity and start to readjust here so we can tackle that youth unemployment in the southern half of Europe. She said very candidly we're not competitive enough just yet. The readjustment is not finished yet. And although it is extremely painful, we <SHOULD> not finish the job until it is <COMPLETELY> done which she suggests you <<COULD>> kick down, the can down the road for another few years but <WOULD> boomerang on the European Union, and she didn't <<<THINK>>> that was the correct strategy, Richard.\" \"\"\"\n",
    "#         ]\n",
    "vectors = []\n",
    "for statement in statements:\n",
    "    sample_text = statement[0]\n",
    "    classification = statement[1]\n",
    "    word = statement[2]\n",
    "    # Encode the text\n",
    "    encoded_input = tokenizer(sample_text, return_tensors='pt')\n",
    "\n",
    "    # Extract input ids and find the indices for \"know\"\n",
    "    input_ids = encoded_input['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    token_indices = [i for i, token in enumerate(tokens) if token == 'know']\n",
    "\n",
    "    # Get the model output\n",
    "    output = model(**encoded_input)\n",
    "\n",
    "    # Extract embeddings for \"know\"\n",
    "    token_embeddings = output.last_hidden_state[0, token_indices, :] if token_indices else None\n",
    "    if token_embeddings is not None:\n",
    "        vectors.append((token_embeddings.mean(dim=0).detach(),classification, word))  # Average if multiple \"know\" tokens and detach\n",
    "    else:\n",
    "        vectors.append((None, None, None))  # No \"know\" found in this sample\n",
    "\n",
    "# Compute cosine similarity between all pairs of vectors\n",
    "if len(vectors) > 1:\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(i + 1, len(vectors)):\n",
    "            if vectors[i] is not None and vectors[j] is not None:\n",
    "                cat1 = vectors[i][1]\n",
    "                cat2 = vectors[j][1]\n",
    "                word = vectors[i][2]\n",
    "                similarity = 1 - cosine(vectors[i][0].numpy(), vectors[j][0].numpy())\n",
    "                print(f\"{word} {i+1},{j+1}: {cat1}/{cat2} Cosine similarity: {similarity:.4f}\")\n",
    "            else:\n",
    "                print(f\"One or both of the texts {i+1} and {j+1} do not contain the token 'know'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
