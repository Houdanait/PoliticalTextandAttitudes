{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Embeddings of Pragmatic Markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "MAX_TOKENS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2017,  0.1938, -0.0742,  ..., -0.0308,  0.4658,  0.6078],\n",
      "         [ 0.6253, -0.1001,  0.3567,  ...,  0.8521,  0.7447,  0.1688],\n",
      "         [-0.0330, -0.3104,  0.5029,  ..., -0.0079,  0.2011,  0.3840],\n",
      "         ...,\n",
      "         [ 0.0347,  0.3426, -0.2987,  ..., -0.2210, -0.2680, -0.7982],\n",
      "         [ 0.8444,  0.1637, -0.2949,  ...,  0.1564, -0.2948, -0.1899],\n",
      "         [ 0.7730,  0.3879,  0.1304,  ...,  0.4094, -0.3863, -0.2330]]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# sample_text = \"\"\"WELL, one THING ABOUT -- WELL, I mean, William and Harry are OBVIOUSLY our main concern.\"\"\"\n",
    "\n",
    "# encoded_input = tokenizer(sample_text, return_tensors = 'pt')\n",
    "\n",
    "# output = model(**encoded_input)\n",
    "\n",
    "# print(output[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flag All Pragmatic Markers to Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: know\n",
      "  hedge: 2\n",
      "  none: 2\n",
      "Term: believe\n",
      "  hedge: 1\n",
      "  none: 1\n",
      "Term: sure\n",
      "  none: 1\n",
      "  hedge: 1\n",
      "Term: tend\n",
      "  hedge: 1\n",
      "  none: 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Load your JSON data\n",
    "with open('data/filtered_utterances_ft_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to hold the count of each value for each term\n",
    "term_counts = {}\n",
    "\n",
    "# Iterate through each item in the JSON data\n",
    "for item in data:\n",
    "    matched_terms = item['matched_terms']\n",
    "    for term, value in matched_terms.items():\n",
    "        if term not in term_counts:\n",
    "            term_counts[term] = Counter()\n",
    "        term_counts[term][value] += 1\n",
    "\n",
    "# Output the results\n",
    "for term, counts in term_counts.items():\n",
    "    if len(counts)>1:\n",
    "        print(f\"Term: {term}\")\n",
    "        for value, count in counts.items():\n",
    "            print(f\"  {value}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all \"Hedges\", all \"Authority\", and all words that are flagged as 2+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: hedge - 32 sample sentences found.\n",
      "Category: authority - 6 sample sentences found.\n",
      "Dual Category Word Count: 10.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get all hedges:\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Define the get_hedge function\n",
    "def get_category(data, category):\n",
    "    samples = []\n",
    "    for item in data:\n",
    "        matched_terms = item['matched_terms']\n",
    "        hedge_terms = {term: value for term, value in matched_terms.items() if value == category}\n",
    "        if hedge_terms:\n",
    "            hedged_item = item.copy()  # Copy the item to avoid modifying the original\n",
    "            hedged_item['matched_terms'] = hedge_terms\n",
    "            samples.append(hedged_item)\n",
    "    print(f\"Category: {category} - {len(samples)} sample sentences found.\")\n",
    "    return samples\n",
    "\n",
    "def get_dual_matches(data):\n",
    "    # Initialize a dictionary to hold the count of each value for each term\n",
    "    term_counts = {}\n",
    "\n",
    "    # Iterate through each item in the JSON data to count the values\n",
    "    for item in data:\n",
    "        matched_terms = item['matched_terms']\n",
    "        for term, value in matched_terms.items():\n",
    "            if term not in term_counts:\n",
    "                term_counts[term] = Counter()\n",
    "            term_counts[term][value] += 1\n",
    "\n",
    "    # Identify terms with 2+ distinct values\n",
    "    dual_terms = {term for term, counts in term_counts.items() if len(counts) > 1}\n",
    "\n",
    "    # Filter the original data to include only those terms with 2+ distinct values\n",
    "    dual_match_items = []\n",
    "    for item in data:\n",
    "        matched_terms = item['matched_terms']\n",
    "        dual_matched_terms = {term: value for term, value in matched_terms.items() if term in dual_terms}\n",
    "        if dual_matched_terms:\n",
    "            dual_item = item.copy()  # Copy the item to avoid modifying the original\n",
    "            dual_item['matched_terms'] = dual_matched_terms\n",
    "            dual_match_items.append(dual_item)\n",
    "    print(f\"Dual Category Word Count: {len(dual_match_items)}.\")\n",
    "    return dual_match_items\n",
    "\n",
    "# Load your JSON data\n",
    "with open('data/filtered_utterances_ft_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Get all items where value is \"hedge\"\n",
    "hedge_items = get_category(data, \"hedge\")\n",
    "# for item in hedge_items[:10]:\n",
    "#     print(item['matched_terms'])\n",
    "    \n",
    "authority_items = get_category(data, \"authority\")\n",
    "# for item in authority_items[:10]:\n",
    "#     print(item['matched_terms'])\n",
    "dual_items = get_dual_matches(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Right. He can't read them all.\"\n",
      "\"Let's get away from the diplomatic wrangling here for a moment. Let's talk ABOUT the military front. Tommy Franks, head of the Central Command, yesterday at the Pentagon says the military is ready to go, is in position now, if GIVEN the order from the White House. To the Pentagon from yesterday, back again today, Barbara Starr for more on this -- Barbara, good morning to you. Turkey, we KNOW, not granting that permission to stage ABOUT 60,000 troops on its territory, which has the Pentagon scrambling right now. We are hearing that some movement, some aircraft carriers in the eastern Mediterranean COULD be on the move. What are we learning ABOUT this?\n",
      "['[CLS]', '\"', 'right', '.', 'he', 'can', \"'\", 't', 'read', 'them', 'all', '.', '\"', '\"', 'let', \"'\", 's', 'get', 'away', 'from', 'the', 'diplomatic', 'wr', '##ang', '##ling', 'here', 'for', 'a', 'moment', '.', 'let', \"'\", 's', 'talk', 'about', 'the', 'military', 'front', '.', 'tommy', 'franks', ',', 'head', 'of', 'the', 'central', 'command', ',', 'yesterday', 'at', 'the', 'pentagon', 'says', 'the', 'military', 'is', 'ready', 'to', 'go', ',', 'is', 'in', 'position', 'now', ',', 'if', 'given', 'the', 'order', 'from', 'the', 'white', 'house', '.', 'to', 'the', 'pentagon', 'from', 'yesterday', ',', 'back', 'again', 'today', ',', 'barbara', 'starr', 'for', 'more', 'on', 'this', '-', '-', 'barbara', ',', 'good', 'morning', 'to', 'you', '.', 'turkey', ',', 'we', 'know', ',', 'not', 'granting', 'that', 'permission', 'to', 'stage', 'about', '60', ',', '000', 'troops', 'on', 'its', 'territory', ',', 'which', 'has', 'the', 'pentagon', 'scrambling', 'right', 'now', '.', 'we', 'are', 'hearing', 'that', 'some', 'movement', ',', 'some', 'aircraft', 'carriers', 'in', 'the', 'eastern', 'mediterranean', 'could', 'be', 'on', 'the', 'move', '.', 'what', 'are', 'we', 'learning', 'about', 'this', '?', '[SEP]']\n",
      "[21]\n",
      "[17, 18]\n",
      "[22, 23, 24]\n",
      "[5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# item = hedge_items[0]\n",
    "# string = f'''\"{item[\"previous_statement\"]}\"\\n\"{item[\"statement\"]}'''\n",
    "# # Remove '<' and '>' characters from the string\n",
    "# string = string.replace(\"<\", \"\")\n",
    "# string = string.replace(\">\", \"\")\n",
    "# print(string)\n",
    "# matched_terms = item['matched_terms']\n",
    "# encoded_input = tokenizer(string, return_tensors='pt')\n",
    "\n",
    "# # Extract input ids and find the indices for \"know\"\n",
    "# input_ids = encoded_input['input_ids'][0]\n",
    "# tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: \"It's important say over and over and over and over and over again on this day, we wish the president well and we wish him a speedy recovery but it is just a fact, it is just a fact that his behavior, the way he has conducted himself in recent days and weeks is directly contrary to the advice of his own experts.\"\n",
      "\"We're ALL praying for the president, praying for the First Lady and any of other folks that MAY end up testing POSITIVE as WELL but it is also true that the president in many ways with his behavior and his rhetoric opened the door to the White House to this virus and remember the White House is both a place of business, it's a workplace and it's also a residence, right? So if you THINK ABOUT those people who come to work EVERY day, they were left very vulnerable because it was a president who didn't really like the look of masks. He thought mask were essentially ugly and didn't want folks AROUND him wearing them and so that is the KIND OF environment that was created. And listen, a virus is ALWAYS looking for host and so this is what ends up happening. Again thoughts and prayers go out to everyone who MIGHT end up being affected by this and the folks who already are. But the president OBVIOUSLY did himself no favors, he did his family members no favors and the people who work AROUND him. He's the boss of those folks and he set a real I THINK leadership pattern and they followed because that is what they thought that this president wanted and the same with his supporters, the same with his family members who went to that a debate and were asked to put on masks and APPARENTLY refused to put on masks because that is what those SORT OF Trump way was in terms of wearing masks. MAYBE it'll CHANGE, MAYBE this will be a wakeup call off for this White House and for this administration. We CERTAINLY HOPE so because you KNOW Americans ALL over you KNOW since March have been changing their ways, their way of life in terms of mask, I've got a mask right here and when I go on to the hallway in CNN, I'm getting my temperature checked and the idea that this White House was so WELL SORT OF lackadaisical in their approach to folks who were coming to work EVERY day. They essentially SEEM to THINK that the testing mechanism made them immune somehow and OBVIOUSLY we find out today sadly that that wasn't the case for the president, for the First Lady and those folks in the White House.\", token length: 515\n",
      "[93]\n",
      "Relevant Token Length: 512\n",
      "[245]\n",
      "Relevant Token Length: 512\n",
      "[362, 368]\n",
      "Relevant Token Length: 512\n",
      "[157, 297, 474]\n",
      "Relevant Token Length: 512\n",
      "[210, 211]\n",
      "Relevant Token Length: 512\n",
      "[351, 352]\n",
      "Relevant Token Length: 512\n",
      "[472]\n",
      "Relevant Token Length: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hedge Vectors: 64\n",
      "Authority Vectors: 64\n"
     ]
    }
   ],
   "source": [
    "def find_phrase_indices(tokens, phrase):\n",
    "    \"\"\"\n",
    "    Find the indices of the phrase in the token list.\n",
    "    \"\"\"\n",
    "    phrase_tokens = tokenizer.tokenize(phrase)\n",
    "    phrase_length = len(phrase_tokens)\n",
    "    \n",
    "    for i in range(len(tokens) - phrase_length + 1):\n",
    "        if tokens[i:i + phrase_length] == phrase_tokens:\n",
    "            return list(range(i, i + phrase_length))\n",
    "    return []\n",
    "\n",
    "def get_phrase_vector(phrase, tokens, output):\n",
    "    \"\"\"\n",
    "    Get the vector for a multi-word phrase by averaging the embeddings of each word and its subwords in the phrase.\n",
    "    \"\"\"\n",
    "    words = phrase.split()\n",
    "    token_indices = []\n",
    "    \n",
    "    for word in words:\n",
    "        word_indices = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Check for exact match or subword match\n",
    "            if token == word or token.lstrip(\"##\") == word:\n",
    "                word_indices.append(i)\n",
    "        token_indices.extend(word_indices)\n",
    "    \n",
    "    if not token_indices:\n",
    "        return None\n",
    "    \n",
    "    # Extract embeddings for specific word indices and average them\n",
    "    token_embeddings = output.last_hidden_state[0, token_indices, :]\n",
    "    phrase_vector = token_embeddings.mean(dim=0).detach()\n",
    "    return phrase_vector\n",
    "    \n",
    "    \n",
    "def get_relevant_segment(tokens, target_indices, max_length=512):\n",
    "    \"\"\"\n",
    "    Get the relevant segment of tokens centered around the target indices within the max_length.\n",
    "    \"\"\"\n",
    "    start = max(0, min(target_indices) - (max_length // 2))\n",
    "    end = start + max_length\n",
    "    if end > len(tokens):\n",
    "        end = len(tokens)\n",
    "        start = max(0, end - max_length)\n",
    "    return tokens[start:end], start, end\n",
    "\n",
    "def get_vectors(item, category, tokenizer=tokenizer, model=model, vectors = []) -> list:\n",
    "    string = f'''\"{item[\"previous_statement\"]}\"\\n\"{item[\"statement\"]}\"'''\n",
    "    string = string.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    \n",
    "    # First tokenize without truncation to get the full token list\n",
    "    full_encoded_input = tokenizer(string, return_tensors='pt')\n",
    "    full_tokens = tokenizer.convert_ids_to_tokens(full_encoded_input['input_ids'][0])\n",
    "    # if len(full_tokens)>MAX_TOKENS:\n",
    "    #     print(f\"{len(full_tokens)} tokens. string: {string}\")\n",
    "    \n",
    "    matched_terms = item['matched_terms']\n",
    "    \n",
    "    for term, value in matched_terms.items():\n",
    "        if value == category:\n",
    "            # Find the indices of the target term or phrase in the full token list\n",
    "            if \" \" in term:  # It's a phrase\n",
    "                term_indices = find_phrase_indices(full_tokens, term)\n",
    "            else:  # It's a single word or subword\n",
    "                term_indices = [i for i, token in enumerate(full_tokens) if token == term or token.lstrip(\"##\") == term]\n",
    "            # if len(full_tokens)>MAX_TOKENS:\n",
    "            #     print(term_indices)\n",
    "\n",
    "            if term_indices:\n",
    "                # Get the relevant segment of tokens\n",
    "                relevant_tokens, start, end = get_relevant_segment(full_tokens, term_indices)\n",
    "                # if len(full_tokens)>MAX_TOKENS:\n",
    "                #     print(\"Relevant Token Length:\",len(relevant_tokens), \"start/end;\", start, end)\n",
    "                \n",
    "                # Retokenize the relevant segment\n",
    "                relevant_string = tokenizer.convert_tokens_to_string(relevant_tokens)\n",
    "                encoded_input = tokenizer(relevant_string, return_tensors='pt', truncation=True, max_length=MAX_TOKENS)\n",
    "                \n",
    "                tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "                output = model(**encoded_input)\n",
    "                \n",
    "                if \" \" in term:  # It's a phrase\n",
    "                    phrase_vector = get_phrase_vector(term, tokens, output)\n",
    "                    if phrase_vector is not None:\n",
    "                        vectors.append((phrase_vector, term, category))\n",
    "                    else:\n",
    "                        vectors.append((None, term, category))\n",
    "                else:  # It's a single word or subword\n",
    "                    word_indices = [i for i, token in enumerate(tokens) if token == term or token.lstrip(\"##\") == term]\n",
    "                    if word_indices:\n",
    "                        word_embeddings = output.last_hidden_state[0, word_indices, :]\n",
    "                        word_vector = word_embeddings.mean(dim=0).detach()\n",
    "                        vectors.append((word_vector, term, category))\n",
    "                    else:\n",
    "                        vectors.append((None, term, category))\n",
    "    return vectors\n",
    "\n",
    "def get_category_vectors(data, category, vectors = []):\n",
    "    for item in data:\n",
    "        # print(\"Analyzing:\", item['matched_terms'])\n",
    "        vectors = get_vectors(item, category, vectors = vectors)\n",
    "    # for vector in vectors:\n",
    "    #     print(vector[1], vector[2], vector[0].shape)\n",
    "    return vectors\n",
    "\n",
    "hedge_vectors = get_category_vectors(hedge_items, \"hedge\")        \n",
    "authority_vectors = get_category_vectors(authority_items, \"authority\")        \n",
    "print(\"Hedge Vectors:\", len(hedge_vectors))\n",
    "print(\"Authority Vectors:\", len(authority_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Vectors for Hedge Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"You know, Salena, as someone who covers the president's base so extensively and so well, answer this for us. Release the memo has become a battle cry for many in the president's base and for many in the right wing. Is there any way that the president does not, cannot release the memo at this point in time, despite the pleas of his own handpicked FBI chief not to do so? Is he stuck? statement: You KNOW, I don't -- I -- because he's such a different KIND OF president and he navigates the presidency in a way, you KNOW, that's not ALWAYS predictable, I mean I COULD see a scenario where, you KNOW, he says, OK, Wray, let's sit down and talk ABOUT this. You KNOW, what's he -- what are the things that you find that are dangerous or COULD put people in jeopardy? What SHOULD we redact? Then I'll release the memo. I mean there is the possibility that that happens. I don't THINK there's any political -- not any. There's ALWAYS some, right? But I don't THINK there's a large political problem if the -- if the president doesn't -- doesn't do that because -- because we're drinking out of a fire hose EVERY day and there's ALWAYS -- there's going to be another crisis one day later, and we're going to forget that this even happened. I don't THINK that's -- I don't THINK that's right or normal, but that's the times that we operate in right now.\", 'hedge', 'know')\n",
      "(\"And you know, gentlemen, every week is a big week in politics in the race for the White House right now. This week is unique, too, because CNN will be hosting this kind of town hall format with, you know, a lot going on. The Trump family on Tuesday night. We have got John Kasich family on Monday night. Still waiting to hear whether, you know, Ted Cruz is going to, you know, dip his toe into it. Bit what kind of expectations would you have, Brian, you know, on how the Kasich family or even the Trump family would want to convey itself on this stage? statement: WELL, I'll tell you, we haven't seen tons of family campaigning. The Kasichs have KIND OF stood by, the governor and his wife, introduced him and a few things. But this will be a chance to actually SORT OF -- voters will have a chance to ask them questions ABOUT, you KNOW, their dad, their husband. They are going to try to give a personal side. They are going to make these guys SEEM like real people. And you KNOW, for Trump, with the large\", 'hedge', 'know')\n",
      "(\"So you would just add a little as your life happened? statement: I was adding to it, and if it was funny and it was true, I      kept it.  And if it wasn't funny and people didn't respond to it, I      dropped it. And so it was really--you KNOW, it was performance art that I      just memorized the BEST parts of.  The ONLY THING I really miss ABOUT      doing it these days is that there's MAYBE a few people in EVERY crowd      that haven't heard it, but it's nothing like having the few hundred or a      few thousand people years and years ago--40 years ago--when nobody had      heard it.  Those moments can't be repeated. And not ONLY that, there were      people singing this song together who politically had nothing in common      and PROBABLY wouldn't have talked to each other.  But to see them ALL      singing together reminded me of the spirit of what the country was ALL      ABOUT, and I was thrilled.  And, unfortunately, those days are gone for      me.  Not--that can't be repeated.\", 'none', 'know')\n",
      "('All right, Ben. You\\'re laughing. statement: Yes, let me jump in here. I THINK that\\'s ABSOLUTELY the most ridiculous THING I have heard since Donald Trump quoting \"National Enquirer\" saying that Ted Cruz\\'s dad was connected to the assassination of JFK. I didn\\'t KNOW it COULD be topped, but congratulations, you just did it. The reason why Paul Ryan is withholding his support is very simple. People -- many people are concerned that Donald Trump is going to do something that will be toxic and it will hurt their re-election campaign. There are many congressmen, many senators, and many people on the state level that are afraid that Donald Trump is going to cross a line and it will hurt everyone who is on a ticket, on a ballot at the same time. What people NEED TO understand ABOUT what Paul Ryan was saying this is, if you want everyone to get behind you, you are going to HAVE TO start acting like more of a statesman. You cannot quote the \"National Enquirer,\" throw it out there two days ago and then EXPECT everyone to get AROUND you just because you\\'re willing to go to the lowest form of politics to try to destroy other Republicans. That\\'s why he said this. And so there does NEED TO be a higher level of professionalism, of decorum from Donald Trump and not ONLY Donald Trump but his campaign staff and others that support him to make SURE that everyone is on the same team. Donald Trump supporters have been vile over the last 48 hours, trashing anyone who was not on the Trump train early on. That is not how you win a general election. I THINK Paul Ryan was standing up for many people that have been taking the heat saying, if you\\'ve got to set the tone, Donald, for your supporters, for your campaign, and you can\\'t go to the \"National Enquirer,\" the lowest form of politics, and EXPECT everybody to jump on board.', 'none', 'know')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load your JSON data\n",
    "with open('data/filtered_utterances_ft_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "word = \"know\"\n",
    "# Filter data to include entries where 'matched_terms' has 'know' as a key\n",
    "filtered_data = [item for item in data if word in item['matched_terms']]\n",
    "statements = []\n",
    "# Optionally, print the filtered data to see the result\n",
    "for item in filtered_data:\n",
    "    # Concatenate strings properly\n",
    "    string = item[\"previous_statement\"] + \" statement: \" + item[\"statement\"]\n",
    "    category = item[\"matched_terms\"][\"know\"]\n",
    "    # Remove '<' and '>' characters from the string\n",
    "    string = string.replace(\"<\", \"\")\n",
    "    string = string.replace(\">\", \"\")\n",
    "    statements.append((string, category, word))\n",
    "for item in statements:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def identify_similar_parts(vectors, threshold=0.5):\n",
    "    num_vectors = len(vectors)\n",
    "    num_dimensions = len(vectors[0])\n",
    "    \n",
    "    # Initialize a list to hold the similarity for each dimension\n",
    "    dimension_similarities = np.zeros(num_dimensions)\n",
    "    \n",
    "    # Compute the cosine similarity for each dimension across all pairs of vectors\n",
    "    for i in range(num_vectors):\n",
    "        for j in range(i + 1, num_vectors):\n",
    "            vec1, vec2 = vectors[i], vectors[j]\n",
    "            for k in range(num_dimensions):\n",
    "                dim_vec1 = np.zeros(num_dimensions)\n",
    "                dim_vec2 = np.zeros(num_dimensions)\n",
    "                dim_vec1[k] = vec1[k]\n",
    "                dim_vec2[k] = vec2[k]\n",
    "                dimension_similarities[k] += cosine_similarity(dim_vec1, dim_vec2)\n",
    "    \n",
    "    # Average the similarity scores for each dimension\n",
    "    dimension_similarities /= (num_vectors * (num_vectors - 1) / 2)\n",
    "    \n",
    "    # Identify dimensions with similarity above the threshold\n",
    "    similar_dimensions = np.where(dimension_similarities > threshold)[0]\n",
    "    \n",
    "    return similar_dimensions, dimension_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know 1,2: hedge/hedge Cosine similarity: 0.8926\n",
      "know 1,3: hedge/none Cosine similarity: 0.7740\n",
      "know 1,4: hedge/none Cosine similarity: 0.3465\n",
      "know 2,3: hedge/none Cosine similarity: 0.8332\n",
      "know 2,4: hedge/none Cosine similarity: 0.4173\n",
      "know 3,4: none/none Cosine similarity: 0.3577\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the pretrained BERT base uncased model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "vectors = []\n",
    "for statement in statements:\n",
    "    sample_text = statement[0]\n",
    "    classification = statement[1]\n",
    "    word = statement[2]\n",
    "    # Encode the text\n",
    "    encoded_input = tokenizer(sample_text, return_tensors='pt')\n",
    "\n",
    "    # Extract input ids and find the indices for \"know\"\n",
    "    input_ids = encoded_input['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    token_indices = [i for i, token in enumerate(tokens) if token == 'know']\n",
    "\n",
    "    # Get the model output\n",
    "    output = model(**encoded_input)\n",
    "\n",
    "    # Extract embeddings for \"know\"\n",
    "    token_embeddings = output.last_hidden_state[0, token_indices, :] if token_indices else None\n",
    "    if token_embeddings is not None:\n",
    "        vectors.append((token_embeddings.mean(dim=0).detach(),classification, word))  # Average if multiple \"know\" tokens and detach\n",
    "    else:\n",
    "        vectors.append((None, None, None))  # No \"know\" found in this sample\n",
    "\n",
    "# Compute cosine similarity between all pairs of vectors\n",
    "if len(vectors) > 1:\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(i + 1, len(vectors)):\n",
    "            if vectors[i] is not None and vectors[j] is not None:\n",
    "                cat1 = vectors[i][1]\n",
    "                cat2 = vectors[j][1]\n",
    "                word = vectors[i][2]\n",
    "                similarity = 1 - cosine(vectors[i][0].numpy(), vectors[j][0].numpy())\n",
    "                print(f\"{word} {i+1},{j+1}: {cat1}/{cat2} Cosine similarity: {similarity:.4f}\")\n",
    "            else:\n",
    "                print(f\"One or both of the texts {i+1} and {j+1} do not contain the token 'know'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
