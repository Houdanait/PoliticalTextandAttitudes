{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('filtered_utterances_ft_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "# Load the lists of pragmatic markers\n",
    "with open('hedging_markers.json', 'r') as file:\n",
    "    hedging_markers = json.load(file)\n",
    "\n",
    "with open('authority_markers.json', 'r') as file:\n",
    "    authority_markers = json.load(file)\n",
    "\n",
    "# Define label mapping\n",
    "label_dict = {'O': 0, 'B-authority': 1, 'I-authority': 2, 'B-hedge': 3, 'I-hedge': 4}\n",
    "label_dict_inv = {v: k for k, v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for token classification\n",
    "class UtteranceTokenDataset(Dataset):\n",
    "    def __init__(self, entries, hedging_markers, authority_markers):\n",
    "        self.entries = entries\n",
    "        self.hedging_markers = set(hedging_markers)\n",
    "        self.authority_markers = set(authority_markers)\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries[idx]\n",
    "        tokens = self.tokenizer.tokenize(entry['statement'])\n",
    "        labels = ['O'] * len(tokens)  # Default label is 'O'\n",
    "\n",
    "        # Apply labels based on hedging and authority markers\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in self.hedging_markers:\n",
    "                labels[i] = 'B-hedge'  # Beginning of a hedging marker\n",
    "            elif token in self.authority_markers:\n",
    "                labels[i] = 'B-authority'  # Beginning of an authority marker\n",
    "\n",
    "        label_ids = [label_dict[label] for label in labels]\n",
    "        \n",
    "        encoding = self.tokenizer(entry['statement'], truncation=True, padding='max_length', max_length=512, is_split_into_words=True)\n",
    "        encoding['labels'] = label_ids\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into a PyTorch Dataset\n",
    "dataset = UtteranceTokenDataset(data, hedging_markers, authority_markers)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Load BERT for token classification\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_dict))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
