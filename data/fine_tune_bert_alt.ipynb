{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                context  label\n",
      "0     Speaker 1: And so you'll see states look to Ma...      0\n",
      "1     Speaker 1: We have changed. The thing is thing...      2\n",
      "2     Speaker 1: And before we head to break, we hav...      0\n",
      "3     Speaker 1: Too bizarre? Speaker 2: Yes, yes. I...      0\n",
      "4     Speaker 1: Mr. President, yesterday Senators L...      0\n",
      "...                                                 ...    ...\n",
      "1156  Speaker 1: And the sales pitch is different th...      0\n",
      "1157  Speaker 1: Everything about that trip was tele...      0\n",
      "1158  Speaker 1: Earlier, we talked with a Utah cong...      0\n",
      "1159  Speaker 1: He does. He's doubled and he's trip...      0\n",
      "1160  Speaker 1: (Through interpreter) The boys here...      0\n",
      "\n",
      "[1161 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('human_annotated_dataset.json', 'r') as f:\n",
    "    json_data = f.read()\n",
    "\n",
    "# Convert JSON to DataFrame\n",
    "data = json.loads(json_data)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Map labels to numerical values\n",
    "label_map = {'hedge': 2, 'authority': 1, 'none': 0}\n",
    "\n",
    "# Function to create separate rows for each marker\n",
    "def create_marker_rows(row):\n",
    "    statement = row['statement']\n",
    "    matched_terms = row['matched_terms']\n",
    "    rows = []\n",
    "    \n",
    "    for term, details in matched_terms.items():\n",
    "        term_upper = term.upper()\n",
    "        label = details['correct']\n",
    "        \n",
    "        # Add start and end markers\n",
    "        context = statement.replace(f'<{term_upper}>', f'[START] {term} [END]')\n",
    "        context = context.replace('<', '').replace('>', '')\n",
    "        if row['previous_statement'] == \"None\":\n",
    "            broad_context = \"Speaker 1: \" + context\n",
    "        else:\n",
    "            broad_context = \"Speaker 1: \" + row['previous_statement'] + \" Speaker 2: \" + context\n",
    "        rows.append({\n",
    "            'transcript_id': row['transcript_id'],\n",
    "            'statement_id': row['statement_id'],\n",
    "            'context': broad_context,\n",
    "            'label': label_map[label]\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# Create a new DataFrame with separate rows for each marker\n",
    "new_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    new_rows.extend(create_marker_rows(row))\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(new_df[['context', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in Random Sampling of \"Non-PPRMs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                context  label\n",
      "0     Speaker 1: And so you'll see states look to Ma...      0\n",
      "1     Speaker 1: We have changed. The thing is thing...      2\n",
      "2     Speaker 1: And before we head to break, we hav...      0\n",
      "3     Speaker 1: Too bizarre? Speaker 2: Yes, yes. I...      0\n",
      "4     Speaker 1: Mr. President, yesterday Senators L...      0\n",
      "...                                                 ...    ...\n",
      "3852  Speaker 1: And you push back on them. And you ...      0\n",
      "3853  Speaker 1: And you push back on them. And you ...      0\n",
      "3854  Speaker 1: It's more than plausible.  It happe...      0\n",
      "3855  Speaker 1: It's more than plausible.  It happe...      0\n",
      "3856  Speaker 1: It's more than plausible.  It happe...      0\n",
      "\n",
      "[3857 rows x 2 columns]\n",
      "Speaker 1: Too bizarre? Speaker 2: Yes, yes. I THINK she's ridiculous and be more realistic. You [START] know [END], we are people, we live on this planet. We want to survive.\n",
      "Total Samples: 3857\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "# Load the non-matching utterances dataset\n",
    "with open('non_matching_utterances_sample.json', 'r') as f:\n",
    "    non_matching_data = json.load(f)\n",
    "non_matching_df = pd.DataFrame(non_matching_data)\n",
    "\n",
    "# Adjust the function to ensure it correctly handles word selection\n",
    "def augment_non_matching_rows(row):\n",
    "    rows = []\n",
    "    statement = row['statement']\n",
    "    words = statement.split()\n",
    "    random_sample_num = min(3, len(words))\n",
    "    random_indices = random.sample(range(len(words)), random_sample_num)\n",
    "    for i in range(random_sample_num):\n",
    "        idx = random_indices[i]\n",
    "        statement = words.copy()\n",
    "        statement[idx] = '[START] ' + statement[idx] + ' [END]'\n",
    "        context = ' '.join(statement)\n",
    "        if row['previous_statement'] == \"None\":\n",
    "            broad_context = \"Speaker 1: \" + context\n",
    "        else:\n",
    "            broad_context = \"Speaker 1: \" + row['previous_statement'] + \" Speaker 2: \" + context\n",
    "        rows.append({\n",
    "            'transcript_id': row['transcript_id'],\n",
    "            'statement_id': str(uuid.uuid4()),\n",
    "            'context': broad_context,\n",
    "            'label': label_map['none']\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# Create a new DataFrame with separate rows for each marker\n",
    "non_match_rows = []\n",
    "for _, row in non_matching_df.iterrows():\n",
    "    non_match_rows.extend(augment_non_matching_rows(row))\n",
    "\n",
    "other_df = pd.DataFrame(non_match_rows)\n",
    "\n",
    "# Combine the existing DataFrame with the augmented data\n",
    "# Ensure new_df is already defined and contains initial data\n",
    "augmented_df = pd.concat([new_df, other_df], ignore_index=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(augmented_df[['context', 'label']])\n",
    "print(augmented_df.head()[\"context\"][3])\n",
    "# for k, v in augmented_df[:1].items():\n",
    "#     print(v)\n",
    "\n",
    "print(\"Total Samples:\", len(augmented_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and Train the Model\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Adjust num_labels to 3\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device='cpu'\n",
    "model.to(device)\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "class CustomSaveCallback(TrainerCallback):\n",
    "    \"A custom callback that saves the model at the end of each epoch with a unique name.\"\n",
    "    def __init__(self, save_path, batch_size, tokenizer):\n",
    "        self.save_path = save_path\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer  # Save the tokenizer as an instance variable\n",
    "\n",
    "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # Format the directory name with the epoch number and batch size\n",
    "        epoch_dir = f\"{self.save_path}/model_checkpoint_epoch-{state.epoch}_batch-{self.batch_size}\"\n",
    "        if not os.path.exists(epoch_dir):\n",
    "            os.makedirs(epoch_dir)\n",
    "        # Save the model and tokenizer in this directory\n",
    "        kwargs['model'].save_pretrained(epoch_dir)\n",
    "        self.tokenizer.save_pretrained(epoch_dir)  # Use the instance variable\n",
    "        print(f\"Saved model and tokenizer to {epoch_dir}\")\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    # Compute F1 for each label individually\n",
    "    f1_per_class = f1_metric.compute(predictions=predictions, references=labels, average=None)\n",
    "    label_f1_scores = f1_per_class['f1']\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall'],\n",
    "        'f1': f1['f1'],\n",
    "        'f1_label_0': label_f1_scores[0],\n",
    "        'f1_label_1': label_f1_scores[1],\n",
    "        'f1_label_2': label_f1_scores[2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Train/Eval/Test, Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of files: 2699 579 579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2699/2699 [00:02<00:00, 1151.59 examples/s]\n",
      "Map: 100%|██████████| 579/579 [00:00<00:00, 1146.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# regen_data = True\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the logging level to ERROR to reduce output clutter\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['context'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "# File paths for saved datasets\n",
    "train_file = 'train_dataset.pkl'\n",
    "eval_file = 'eval_dataset.pkl'\n",
    "test_file = 'test_dataset.pkl'\n",
    "\n",
    "# Function to save a dataset\n",
    "def save_dataset(data, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "# Function to load a dataset\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "# Check if regeneration of data is needed\n",
    "regen_data = True\n",
    "if regen_data:\n",
    "    # Split the data into training and evaluation sets\n",
    "    train_eval_df, test_df = train_test_split(augmented_df, test_size=0.15, random_state=42, stratify=augmented_df['label'])\n",
    "    # Ensure to use train_eval_df for further splitting and stratification\n",
    "    train_df, eval_df = train_test_split(train_eval_df, test_size=0.1764705882352941, random_state=42, stratify=train_eval_df['label'])\n",
    "    save_dataset(train_df, train_file)\n",
    "    save_dataset(eval_df, eval_file)\n",
    "    save_dataset(test_df, test_file)\n",
    "    print(\"Length of files:\", len(train_df), len(eval_df), len(test_df))\n",
    "elif os.path.exists(train_file) and os.path.exists(eval_file) and os.path.exists(test_file):\n",
    "    # Load the datasets\n",
    "    train_df = load_dataset(train_file)\n",
    "    eval_df = load_dataset(eval_file)\n",
    "    test_df = load_dataset(test_file)\n",
    "    print(\"Length of files:\", len(train_df), len(eval_df), len(test_df))\n",
    "else:\n",
    "    # Split the data into training and evaluation sets\n",
    "    train_eval_df, test_df = train_test_split(augmented_df, test_size=0.15, random_state=42, stratify = augmented_df['label'])\n",
    "    train_df, eval_df = train_test_split(train_eval_df, test_size=0.1764705882352941, random_state=42, stratify = train_eval_df['label'])\n",
    "    save_dataset(train_df, train_file)\n",
    "    save_dataset(eval_df, eval_file)\n",
    "    save_dataset(test_df, test_file)\n",
    "    print(\"Length of files:\", len(train_df), len(eval_df), len(test_df))\n",
    "    \n",
    "from datasets import Dataset\n",
    "# Convert DataFrame to Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_tokenized = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\", config=\"multiclass\")\n",
    "recall_metric = evaluate.load(\"recall\", config=\"multiclass\")\n",
    "f1_metric = evaluate.load(\"f1\", config=\"multiclass\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: 0    2345\n",
      "1      45\n",
      "2     309\n",
      "Name: label, dtype: int64\n",
      "Class Weights: tensor([ 0.3837, 19.9926,  2.9115], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Example class counts: You might need to calculate or provide actual counts\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "print(\"Class counts:\", class_counts)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "# Print class weights to verify\n",
    "print(\"Class Weights:\", class_weights_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Weighted Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model, class_weights):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = model  # this should be an instance of a Hugging Face PreTrainedModel\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        if labels is not None:\n",
    "            self.class_weights = self.class_weights.to(self.model.device)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            return (loss, logits)\n",
    "        return logits\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\"Save the contained PreTrainedModel to a directory.\"\"\"\n",
    "        self.model.save_pretrained(save_directory)\n",
    "\n",
    "    def from_pretrained(self, load_directory):\n",
    "        \"\"\"Load the contained PreTrainedModel from a directory.\"\"\"\n",
    "        self.model = self.model.from_pretrained(load_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Weighted Loss Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2316' max='2316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2316/2316 08:13, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Label 0</th>\n",
       "      <th>F1 Label 1</th>\n",
       "      <th>F1 Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.726545</td>\n",
       "      <td>weighted_loss</td>\n",
       "      <td>0.965026</td>\n",
       "      <td>0.859481</td>\n",
       "      <td>0.779773</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.981604</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>1.051479</td>\n",
       "      <td>weighted_loss</td>\n",
       "      <td>0.936528</td>\n",
       "      <td>0.559258</td>\n",
       "      <td>0.602707</td>\n",
       "      <td>0.578981</td>\n",
       "      <td>0.970105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.766839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.392900</td>\n",
       "      <td>0.472366</td>\n",
       "      <td>weighted_loss</td>\n",
       "      <td>0.954663</td>\n",
       "      <td>0.775643</td>\n",
       "      <td>0.864396</td>\n",
       "      <td>0.804166</td>\n",
       "      <td>0.974589</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.861390</td>\n",
       "      <td>weighted_loss</td>\n",
       "      <td>0.965026</td>\n",
       "      <td>0.876587</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.812215</td>\n",
       "      <td>0.981495</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.883721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.628369</td>\n",
       "      <td>weighted_loss</td>\n",
       "      <td>0.961140</td>\n",
       "      <td>0.804587</td>\n",
       "      <td>0.841735</td>\n",
       "      <td>0.817314</td>\n",
       "      <td>0.978439</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.598807</td>\n",
       "      <td>weighted_loss</td>\n",
       "      <td>0.962435</td>\n",
       "      <td>0.821277</td>\n",
       "      <td>0.838941</td>\n",
       "      <td>0.827199</td>\n",
       "      <td>0.979955</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.880952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-1.0_batch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"weighted_loss\" of type <class 'str'> for key \"eval/model_name\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-2.0_batch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwise/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Trainer is attempting to log a value of \"weighted_loss\" of type <class 'str'> for key \"eval/model_name\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-3.0_batch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"weighted_loss\" of type <class 'str'> for key \"eval/model_name\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-4.0_batch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"weighted_loss\" of type <class 'str'> for key \"eval/model_name\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-5.0_batch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"weighted_loss\" of type <class 'str'> for key \"eval/model_name\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-6.0_batch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"weighted_loss\" of type <class 'str'> for key \"eval/model_name\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='97' max='97' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [97/97 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"weighted_loss\" of type <class 'str'> for key \"eval/model_name\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.5988073945045471, 'eval_model_name': 'weighted_loss', 'eval_accuracy': 0.9624352331606217, 'eval_precision': 0.8212771203155818, 'eval_recall': 0.8389411135312774, 'eval_f1': 0.8271991642316502, 'eval_f1_label_0': 0.9799554565701559, 'eval_f1_label_1': 0.6206896551724138, 'eval_f1_label_2': 0.8809523809523809, 'eval_runtime': 6.3406, 'eval_samples_per_second': 121.755, 'eval_steps_per_second': 15.298, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and ensure it and its weights are on the correct device\n",
    "model_with_loss = CustomModel(original_model.to(device), class_weights_tensor.to(device))\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the custom model\n",
    "trainer = Trainer(\n",
    "    model=model_with_loss,  # Ensure this is your custom model accepting weights\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,  # Ensure datasets are correctly tokenized\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,  # Custom metrics function if needed\n",
    "    callbacks=[CustomSaveCallback('./weighted_checkpoints', training_args.per_device_train_batch_size, tokenizer)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "import datetime\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/weighted_model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/weighted_tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3085/3085 [00:02<00:00, 1116.87 examples/s]\n",
      "Map: 100%|██████████| 772/772 [00:00<00:00, 1100.17 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='194' max='97' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [97/97 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.19305863976478577, 'eval_accuracy': 0.9637305699481865, 'eval_precision': 0.8137048348023298, 'eval_recall': 0.8526023157170698, 'eval_f1': 0.8309050805087862, 'eval_f1_label_0': 0.9805970149253731, 'eval_f1_label_1': 0.6206896551724138, 'eval_f1_label_2': 0.8914285714285715, 'eval_runtime': 6.9171, 'eval_samples_per_second': 111.607, 'eval_steps_per_second': 14.023}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2316' max='2316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2316/2316 08:46, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Label 0</th>\n",
       "      <th>F1 Label 1</th>\n",
       "      <th>F1 Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.254314</td>\n",
       "      <td>0.957254</td>\n",
       "      <td>0.766076</td>\n",
       "      <td>0.676215</td>\n",
       "      <td>0.713160</td>\n",
       "      <td>0.977223</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.876543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.244236</td>\n",
       "      <td>0.957254</td>\n",
       "      <td>0.850391</td>\n",
       "      <td>0.651071</td>\n",
       "      <td>0.695954</td>\n",
       "      <td>0.977256</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.860606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.235526</td>\n",
       "      <td>0.966321</td>\n",
       "      <td>0.865899</td>\n",
       "      <td>0.753019</td>\n",
       "      <td>0.792925</td>\n",
       "      <td>0.982222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.246897</td>\n",
       "      <td>0.963731</td>\n",
       "      <td>0.828704</td>\n",
       "      <td>0.852602</td>\n",
       "      <td>0.839515</td>\n",
       "      <td>0.979136</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.213502</td>\n",
       "      <td>0.962435</td>\n",
       "      <td>0.829844</td>\n",
       "      <td>0.864085</td>\n",
       "      <td>0.842272</td>\n",
       "      <td>0.979198</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.880952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.233005</td>\n",
       "      <td>0.963731</td>\n",
       "      <td>0.805823</td>\n",
       "      <td>0.846020</td>\n",
       "      <td>0.820236</td>\n",
       "      <td>0.980655</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.899408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-1.0_batch-8\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-2.0_batch-8\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-3.0_batch-8\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-4.0_batch-8\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-5.0_batch-8\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-6.0_batch-8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='97' max='97' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [97/97 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in directory: ./models/model_epochs-6_batch-8_2024-06-05_12-17-29\n",
      "Tokenizer saved in directory: ./models/tokenizer_epochs-6_batch-8_2024-06-05_12-17-29\n",
      "Evaluation results: {'eval_loss': 0.23300491273403168, 'eval_accuracy': 0.9637305699481865, 'eval_precision': 0.8058230758412366, 'eval_recall': 0.8460201001184608, 'eval_f1': 0.8202360690729177, 'eval_f1_label_0': 0.9806547619047619, 'eval_f1_label_1': 0.5806451612903226, 'eval_f1_label_2': 0.8994082840236687, 'eval_runtime': 6.7885, 'eval_samples_per_second': 113.722, 'eval_steps_per_second': 14.289, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments and initialize the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CustomSaveCallback('./unique_checkpoints', training_args.per_device_train_batch_size, tokenizer)]  # Pass the tokenizer here\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: Evaluate the model after training is complete\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "import datetime\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/standard_model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/standard_tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Standard with Larger Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments and initialize the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CustomSaveCallback('./unique_checkpoints', training_args.per_device_train_batch_size, tokenizer)]  # Pass the tokenizer here\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: Evaluate the model after training is complete\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Final Model Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compute the confusion matrix\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(\u001b[43mtrue_labels\u001b[49m, predictions, labels\u001b[38;5;241m=\u001b[39munique_labels)\n\u001b[1;32m      6\u001b[0m display_labels \u001b[38;5;241m=\u001b[39m [reversed_label_map[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m unique_labels]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display the confusion matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions, labels=unique_labels)\n",
    "display_labels = [reversed_label_map[label] for label in unique_labels]\n",
    "\n",
    "# Display the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "disp.plot(values_format='d', cmap='Blues', ax=ax)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
