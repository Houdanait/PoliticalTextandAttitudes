{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                context  label\n",
      "0     I'm [START] THINKING [END] now of issues that ...      0\n",
      "1     We all share your frustration. Thank you, Pat....      2\n",
      "2     Good morning, everyone. We'll be right back. I...      0\n",
      "3     Yes, yes. I THINK she's ridiculous and be more...      0\n",
      "4     Thanks for the question. That was on OBVIOUSLY...      0\n",
      "...                                                 ...    ...\n",
      "1156  It's very worrisome. Thanks very much. Dan Lot...      0\n",
      "1157  \"Nixon,\" a fantastic new flick and it's [START...      0\n",
      "1158  Listen, I THINK that ultimately all the nation...      0\n",
      "1159  All right. Jim Sciutto, thank you. In OUTFRONT...      0\n",
      "1160  Spanish Media say that back in 2004, police co...      0\n",
      "\n",
      "[1161 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('human_annotated_dataset.json', 'r') as f:\n",
    "    json_data = f.read()\n",
    "\n",
    "# Convert JSON to DataFrame\n",
    "data = json.loads(json_data)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to create separate rows for each marker\n",
    "def create_marker_rows(row):\n",
    "    statement = row['statement']\n",
    "    matched_terms = row['matched_terms']\n",
    "    rows = []\n",
    "    for term, details in matched_terms.items():\n",
    "        label = details['correct']\n",
    "        context = statement.replace(f'<{term.upper()}>', f'[START] {term.upper()} [END]')\n",
    "        context = context.replace('<', '').replace('>', '')\n",
    "        rows.append({\n",
    "            'transcript_id': row['transcript_id'],\n",
    "            'statement_id': row['statement_id'],\n",
    "            'context': context,\n",
    "            'label': label_map[label]\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# Map labels to numerical values (e.g., 'hedge' -> 0, 'authority' -> 1, 'none' -> 2)\n",
    "label_map = {'hedge': 2, 'authority': 1, 'none': 0}\n",
    "\n",
    "\n",
    "# Create a new DataFrame with separate rows for each marker\n",
    "new_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    new_rows.extend(create_marker_rows(row))\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(new_df[['context', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and Train the Model\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Adjust num_labels to 3\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device='cpu'\n",
    "model.to(device)\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwise/.local/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/mwise/.local/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/mwise/.local/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/mwise/.local/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 928/928 [00:00<00:00, 945.23 examples/s]\n",
      "Map: 100%|██████████| 233/233 [00:00<00:00, 999.89 examples/s] \n",
      "/home/mwise/.local/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/mwise/.local/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/mwise/.local/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/mwise/.local/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='696' max='696' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [696/696 02:33, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Label 0</th>\n",
       "      <th>F1 Label 1</th>\n",
       "      <th>F1 Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.182900</td>\n",
       "      <td>0.612327</td>\n",
       "      <td>0.866953</td>\n",
       "      <td>0.755996</td>\n",
       "      <td>0.820656</td>\n",
       "      <td>0.778503</td>\n",
       "      <td>0.904215</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.875740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.645470</td>\n",
       "      <td>0.884120</td>\n",
       "      <td>0.826121</td>\n",
       "      <td>0.810748</td>\n",
       "      <td>0.818102</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.877193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.699901</td>\n",
       "      <td>0.862661</td>\n",
       "      <td>0.752305</td>\n",
       "      <td>0.794107</td>\n",
       "      <td>0.767005</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.872727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.426300</td>\n",
       "      <td>0.854361</td>\n",
       "      <td>0.854077</td>\n",
       "      <td>0.729109</td>\n",
       "      <td>0.723815</td>\n",
       "      <td>0.725157</td>\n",
       "      <td>0.890511</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.871166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.840208</td>\n",
       "      <td>0.866953</td>\n",
       "      <td>0.790317</td>\n",
       "      <td>0.819287</td>\n",
       "      <td>0.802491</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.874251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.159900</td>\n",
       "      <td>0.692714</td>\n",
       "      <td>0.905579</td>\n",
       "      <td>0.815327</td>\n",
       "      <td>0.869995</td>\n",
       "      <td>0.837357</td>\n",
       "      <td>0.927757</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.917647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in directory: ./model_epochs-6_batch-8_2024-06-05_00-15-10\n",
      "Tokenizer saved in directory: ./tokenizer_epochs-6_batch-8_2024-06-05_00-15-10\n",
      "Evaluation results: {'eval_loss': 0.6927139163017273, 'eval_accuracy': 0.9055793991416309, 'eval_precision': 0.8153267784846733, 'eval_recall': 0.8699947543276796, 'eval_f1': 0.8373567931608639, 'eval_f1_label_0': 0.9277566539923955, 'eval_f1_label_1': 0.6666666666666666, 'eval_f1_label_2': 0.9176470588235294, 'eval_runtime': 2.9543, 'eval_samples_per_second': 78.867, 'eval_steps_per_second': 10.155, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "precision_metric = load_metric(\"precision\", config_name=\"multiclass\")\n",
    "recall_metric = load_metric(\"recall\", config_name=\"multiclass\")\n",
    "f1_metric = load_metric(\"f1\", config_name=\"multiclass\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['context'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Split the data into training and evaluation sets\n",
    "train_df, eval_df = train_test_split(new_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert DataFrame to Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_tokenized = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load evaluation metric\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    # Compute F1 for each label individually\n",
    "    f1_per_class = f1_metric.compute(predictions=predictions, references=labels, average=None)\n",
    "    label_f1_scores = f1_per_class['f1']\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall'],\n",
    "        'f1': f1['f1'],\n",
    "        'f1_label_0': label_f1_scores[0],\n",
    "        'f1_label_1': label_f1_scores[1],\n",
    "        'f1_label_2': label_f1_scores[2]\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions, labels=unique_labels)\n",
    "display_labels = [reversed_label_map[label] for label in unique_labels]\n",
    "\n",
    "# Display the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "disp.plot(values_format='d', cmap='Blues', ax=ax)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
