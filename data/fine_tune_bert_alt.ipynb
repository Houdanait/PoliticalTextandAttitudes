{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                context  label\n",
      "0     Speaker 1: A moderator, first of all. Howie, y...      2\n",
      "1     Speaker 1: A moderator, first of all. Howie, y...      2\n",
      "2     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "3     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "4     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "...                                                 ...    ...\n",
      "1332  Speaker 1: We were looking at some other thing...      0\n",
      "1333  Speaker 1: I cannot speculate on anything past...      0\n",
      "1334  Speaker 1: Yes. Look, as I said before, I thin...      2\n",
      "1335  Speaker 1: Yes. Look, as I said before, I thin...      0\n",
      "1336  Speaker 1: Yes. Look, as I said before, I thin...      2\n",
      "\n",
      "[1337 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('human_annotated_dataset.json', 'r') as f:\n",
    "    json_data = f.read()\n",
    "\n",
    "# Convert JSON to DataFrame\n",
    "data = json.loads(json_data)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Map labels to numerical values\n",
    "label_map = {'hedge': 2, 'authority': 1, 'none': 0}\n",
    "\n",
    "# Function to create separate rows for each marker\n",
    "def create_marker_rows(row):\n",
    "    statement = row['statement']\n",
    "    matched_terms = row['matched_terms']\n",
    "    rows = []\n",
    "    \n",
    "    for term, details in matched_terms.items():\n",
    "        term_upper = term.upper()\n",
    "        label = details['correct']\n",
    "        \n",
    "        # Add start and end markers\n",
    "        context = statement.replace(f'<{term_upper}>', f'[START] {term} [END]')\n",
    "        context = context.replace('<', '').replace('>', '')\n",
    "        if row['previous_statement'] == \"None\":\n",
    "            broad_context = \"Speaker 1: \" + context\n",
    "        else:\n",
    "            broad_context = \"Speaker 1: \" + row['previous_statement'] + \" Speaker 2: \" + context\n",
    "        rows.append({\n",
    "            'transcript_id': row['transcript_id'],\n",
    "            'statement_id': row['statement_id'],\n",
    "            'context': broad_context,\n",
    "            'label': label_map[label]\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# Create a new DataFrame with separate rows for each marker\n",
    "new_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    new_rows.extend(create_marker_rows(row))\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(new_df[['context', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in Random Sampling of \"Non-PPRMs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "                                                context  label\n",
      "0     Speaker 1: A moderator, first of all. Howie, y...      2\n",
      "1     Speaker 1: A moderator, first of all. Howie, y...      2\n",
      "2     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "3     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "4     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "...                                                 ...    ...\n",
      "1908  Speaker 1: That same month, according to offic...      0\n",
      "1909  Speaker 1: The time for excuses for      terro...      0\n",
      "1910  Speaker 1: The time for excuses for      terro...      0\n",
      "1911  Speaker 1: The weather is great. San Jose is a...      0\n",
      "1912  Speaker 1: The weather is great. San Jose is a...      0\n",
      "\n",
      "[1913 rows x 2 columns]\n",
      "Speaker 1: A moderator, first of all. Howie, you know, some people have compared her to Nurse Ratched. Really what you have here is a junior high school sort of history teacher who doesn't really know the lesson plan and is sort of faking it by being didactic and over the top and assertive. She -- you know, there was a point where Mitt Romney answered a question and she said -- it was on taxes. And she said, \"OK, a little snappier, gentlemen.\" And you expected her to clap her hands twice like a nanny, or maybe a riding crop would have been a little bit better. She was so -- listen, you know, she had such a tin ear for the television medium. You thought they wouldn't bring her back the next day for the Democrats, but they did. Speaker 2: You KNOW, it's funny. When TV anchors MODERATE these things, whether it's Russert or Blitzer or Chris Matthews, people SOMETIMES say they're too aggressive, they turn it into a talk show, but MAYBE there's something to having somebody who knows something ABOUT television? WELL, she was as aggressive, just in a very\n",
      "Total Samples: 1913\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "# Load the non-matching utterances dataset\n",
    "with open('non_matching_utterances_sample.json', 'r') as f:\n",
    "    non_matching_data = json.load(f)\n",
    "print(len(non_matching_data))\n",
    "non_matching_df = pd.DataFrame(non_matching_data)\n",
    "\n",
    "# Adjust the function to ensure it correctly handles word selection\n",
    "def augment_non_matching_rows(row):\n",
    "    rows = []\n",
    "    statement = row['statement']\n",
    "    words = statement.split()\n",
    "    random_sample_num = min(2, len(words))\n",
    "    random_indices = random.sample(range(len(words)), random_sample_num)\n",
    "    for i in range(random_sample_num):\n",
    "        idx = random_indices[i]\n",
    "        statement = words.copy()\n",
    "        statement[idx] = '[START] ' + statement[idx] + ' [END]'\n",
    "        context = ' '.join(statement)\n",
    "        if row['previous_statement'] == \"None\":\n",
    "            broad_context = \"Speaker 1: \" + context\n",
    "        else:\n",
    "            broad_context = \"Speaker 1: \" + row['previous_statement'] + \" Speaker 2: \" + context\n",
    "        rows.append({\n",
    "            'transcript_id': row['transcript_id'],\n",
    "            'statement_id': str(uuid.uuid4()),\n",
    "            'context': broad_context,\n",
    "            'label': label_map['none']\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# Create a new DataFrame with separate rows for each marker\n",
    "non_match_rows = []\n",
    "for _, row in non_matching_df.iterrows():\n",
    "    non_match_rows.extend(augment_non_matching_rows(row))\n",
    "\n",
    "other_df = pd.DataFrame(non_match_rows)\n",
    "\n",
    "# Combine the existing DataFrame with the augmented data\n",
    "# Ensure new_df is already defined and contains initial data\n",
    "augmented_df = pd.concat([new_df, other_df], ignore_index=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(augmented_df[['context', 'label']])\n",
    "print(augmented_df.head()[\"context\"][3])\n",
    "# for k, v in augmented_df[:1].items():\n",
    "#     print(v)\n",
    "\n",
    "print(\"Total Samples:\", len(augmented_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwise/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-05 23:22:46.288021: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-05 23:22:46.329978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-05 23:22:47.008891: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and Train the Model\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Adjust num_labels to 3\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device='cpu'\n",
    "model.to(device)\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "class CustomSaveCallback(TrainerCallback):\n",
    "    \"A custom callback that saves the model at the end of each epoch with a unique name.\"\n",
    "    def __init__(self, save_path, batch_size, tokenizer):\n",
    "        self.save_path = save_path\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer  # Save the tokenizer as an instance variable\n",
    "\n",
    "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # Format the directory name with the epoch number and batch size\n",
    "        epoch_dir = f\"{self.save_path}/model_checkpoint_epoch-{state.epoch}_batch-{self.batch_size}\"\n",
    "        if not os.path.exists(epoch_dir):\n",
    "            os.makedirs(epoch_dir)\n",
    "        # Save the model and tokenizer in this directory\n",
    "        kwargs['model'].save_pretrained(epoch_dir)\n",
    "        self.tokenizer.save_pretrained(epoch_dir)  # Use the instance variable\n",
    "        print(f\"Saved model and tokenizer to {epoch_dir}\")\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    # Compute F1 for each label individually\n",
    "    f1_per_class = f1_metric.compute(predictions=predictions, references=labels, average=None)\n",
    "    label_f1_scores = f1_per_class['f1']\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall'],\n",
    "        'f1': f1['f1'],\n",
    "        'f1_label_0': label_f1_scores[0],\n",
    "        'f1_label_1': label_f1_scores[1],\n",
    "        'f1_label_2': label_f1_scores[2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on non-Augmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Train/Eval/Test, Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating new datasets\n",
      "Length of files: 935 201 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 935/935 [00:01<00:00, 613.72 examples/s]\n",
      "Map: 100%|██████████| 201/201 [00:00<00:00, 621.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "regen_data = True\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the logging level to ERROR to reduce output clutter\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['context'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "# File paths for saved datasets\n",
    "train_file = 'train_dataset_nonaug.pkl'\n",
    "eval_file = 'eval_dataset_nonaug.pkl'\n",
    "test_file = 'test_dataset_nonaug.pkl'\n",
    "\n",
    "# Function to save a dataset\n",
    "def save_dataset(data, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "# Function to load a dataset\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "# Check if regeneration of data is needed\n",
    "if regen_data:\n",
    "    print(\"Generating new datasets\")\n",
    "    # Split the data into training and evaluation sets\n",
    "    train_eval_df, test_df = train_test_split(new_df, test_size=0.15, random_state=42, stratify=new_df['label'])\n",
    "    # Ensure to use train_eval_df for further splitting and stratification\n",
    "    train_df, eval_df = train_test_split(train_eval_df, test_size=0.1764705882352941, random_state=42, stratify=train_eval_df['label'])\n",
    "    save_dataset(train_df, train_file)\n",
    "    save_dataset(eval_df, eval_file)\n",
    "    save_dataset(test_df, test_file)\n",
    "    print(\"Length of files:\", len(train_df), len(eval_df), len(test_df))\n",
    "elif os.path.exists(train_file) and os.path.exists(eval_file) and os.path.exists(test_file):\n",
    "    # Load the datasets\n",
    "    train_df = load_dataset(train_file)\n",
    "    eval_df = load_dataset(eval_file)\n",
    "    test_df = load_dataset(test_file)\n",
    "    print(\"Length of files:\", len(train_df), len(eval_df), len(test_df))\n",
    "else:\n",
    "    # Split the data into training and evaluation sets\n",
    "    train_eval_df, test_df = train_test_split(augmented_df, test_size=0.15, random_state=42, stratify = augmented_df['label'])\n",
    "    train_df, eval_df = train_test_split(train_eval_df, test_size=0.1764705882352941, random_state=42, stratify = train_eval_df['label'])\n",
    "    save_dataset(train_df, train_file)\n",
    "    save_dataset(eval_df, eval_file)\n",
    "    save_dataset(test_df, test_file)\n",
    "    print(\"Length of files:\", len(train_df), len(eval_df), len(test_df))\n",
    "    \n",
    "from datasets import Dataset\n",
    "# Convert DataFrame to Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_tokenized = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\", config=\"multiclass\")\n",
    "recall_metric = evaluate.load(\"recall\", config=\"multiclass\")\n",
    "f1_metric = evaluate.load(\"f1\", config=\"multiclass\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: 0    490\n",
      "1    136\n",
      "2    309\n",
      "Name: label, dtype: int64\n",
      "Class Weights: tensor([0.6361, 2.2917, 1.0086], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Example class counts: You might need to calculate or provide actual counts\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "print(\"Class counts:\", class_counts)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "# Print class weights to verify\n",
    "print(\"Class Weights:\", class_weights_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Weighted Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model, class_weights):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = model  # this should be an instance of a Hugging Face PreTrainedModel\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        if labels is not None:\n",
    "            self.class_weights = self.class_weights.to(self.model.device)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            return (loss, logits)\n",
    "        return logits\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\"Save the contained PreTrainedModel to a directory.\"\"\"\n",
    "        self.model.save_pretrained(save_directory)\n",
    "\n",
    "    def from_pretrained(self, load_directory):\n",
    "        \"\"\"Load the contained PreTrainedModel from a directory.\"\"\"\n",
    "        self.model = self.model.from_pretrained(load_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Weighted Loss Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='702' max='702' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [702/702 02:51, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Label 0</th>\n",
       "      <th>F1 Label 1</th>\n",
       "      <th>F1 Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.098249</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.419697</td>\n",
       "      <td>0.338867</td>\n",
       "      <td>0.294204</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.179487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.062014</td>\n",
       "      <td>0.378109</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.408345</td>\n",
       "      <td>0.340885</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.581342</td>\n",
       "      <td>0.781095</td>\n",
       "      <td>0.746312</td>\n",
       "      <td>0.788667</td>\n",
       "      <td>0.762189</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636229</td>\n",
       "      <td>0.830846</td>\n",
       "      <td>0.812913</td>\n",
       "      <td>0.827737</td>\n",
       "      <td>0.819110</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.834532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.797800</td>\n",
       "      <td>0.602664</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.817749</td>\n",
       "      <td>0.836659</td>\n",
       "      <td>0.826446</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.827068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.797800</td>\n",
       "      <td>0.767198</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.834954</td>\n",
       "      <td>0.862066</td>\n",
       "      <td>0.846050</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.842857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-1.0_batch-8\n",
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-2.0_batch-8\n",
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-3.0_batch-8\n",
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-4.0_batch-8\n",
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-5.0_batch-8\n",
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-6.0_batch-8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.7671979665756226, 'eval_accuracy': 0.8557213930348259, 'eval_precision': 0.8349543622569938, 'eval_recall': 0.8620656795673062, 'eval_f1': 0.8460503881839826, 'eval_f1_label_0': 0.8756218905472637, 'eval_f1_label_1': 0.819672131147541, 'eval_f1_label_2': 0.8428571428571429, 'eval_runtime': 2.7525, 'eval_samples_per_second': 73.024, 'eval_steps_per_second': 9.446, 'epoch': 6.0}\n",
      "Model saved in directory: ./models/weighted_model_epochs-6_batch-8_2024-06-05_23-27-25\n",
      "Tokenizer saved in directory: ./models/weighted_tokenizer_epochs-6_batch-8_2024-06-05_23-27-25\n",
      "Evaluation results: {'eval_loss': 0.7671979665756226, 'eval_accuracy': 0.8557213930348259, 'eval_precision': 0.8349543622569938, 'eval_recall': 0.8620656795673062, 'eval_f1': 0.8460503881839826, 'eval_f1_label_0': 0.8756218905472637, 'eval_f1_label_1': 0.819672131147541, 'eval_f1_label_2': 0.8428571428571429, 'eval_runtime': 2.7525, 'eval_samples_per_second': 73.024, 'eval_steps_per_second': 9.446, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Initialize the model and ensure it and its weights are on the correct device\n",
    "model_with_loss = CustomModel(original_model.to(device), class_weights_tensor.to(device))\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the custom model\n",
    "trainer = Trainer(\n",
    "    model=model_with_loss,  # Ensure this is your custom model accepting weights\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,  # Ensure datasets are correctly tokenized\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,  # Custom metrics function if needed\n",
    "    callbacks=[CustomSaveCallback('./weighted_checkpoints', training_args.per_device_train_batch_size, tokenizer)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "import datetime\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/weighted_model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/weighted_tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.1765142679214478, 'eval_accuracy': 0.1691542288557214, 'eval_precision': 0.35546642899584074, 'eval_recall': 0.34261285595052676, 'eval_f1': 0.12447226222566866, 'eval_f1_label_0': 0.0847457627118644, 'eval_f1_label_1': 0.25925925925925924, 'eval_f1_label_2': 0.029411764705882353, 'eval_runtime': 3.2315, 'eval_samples_per_second': 62.2, 'eval_steps_per_second': 8.046}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='702' max='702' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [702/702 02:48, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Label 0</th>\n",
       "      <th>F1 Label 1</th>\n",
       "      <th>F1 Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.985118</td>\n",
       "      <td>0.527363</td>\n",
       "      <td>0.175788</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.230185</td>\n",
       "      <td>0.690554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909553</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.434147</td>\n",
       "      <td>0.487898</td>\n",
       "      <td>0.430225</td>\n",
       "      <td>0.692737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.496714</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.842626</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686811</td>\n",
       "      <td>0.810945</td>\n",
       "      <td>0.773249</td>\n",
       "      <td>0.805629</td>\n",
       "      <td>0.785672</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.815385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>0.643786</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.868240</td>\n",
       "      <td>0.822950</td>\n",
       "      <td>0.841791</td>\n",
       "      <td>0.875576</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>0.724426</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.854400</td>\n",
       "      <td>0.815422</td>\n",
       "      <td>0.830757</td>\n",
       "      <td>0.858491</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-1.0_batch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwise/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-2.0_batch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwise/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-3.0_batch-8\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-4.0_batch-8\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-5.0_batch-8\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-6.0_batch-8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.7244257926940918, 'eval_accuracy': 0.8407960199004975, 'eval_precision': 0.854400236988424, 'eval_recall': 0.8154216202361941, 'eval_f1': 0.8307566100839275, 'eval_f1_label_0': 0.8584905660377359, 'eval_f1_label_1': 0.8076923076923077, 'eval_f1_label_2': 0.8260869565217391, 'eval_runtime': 1.7611, 'eval_samples_per_second': 114.135, 'eval_steps_per_second': 14.764, 'epoch': 6.0}\n",
      "Model saved in directory: ./models/standard_model_epochs-6_batch-8_2024-06-05_23-33-06\n",
      "Tokenizer saved in directory: ./models/standard_tokenizer_epochs-6_batch-8_2024-06-05_23-33-06\n",
      "Evaluation results: {'eval_loss': 0.7244257926940918, 'eval_accuracy': 0.8407960199004975, 'eval_precision': 0.854400236988424, 'eval_recall': 0.8154216202361941, 'eval_f1': 0.8307566100839275, 'eval_f1_label_0': 0.8584905660377359, 'eval_f1_label_1': 0.8076923076923077, 'eval_f1_label_2': 0.8260869565217391, 'eval_runtime': 1.7611, 'eval_samples_per_second': 114.135, 'eval_steps_per_second': 14.764, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments and initialize the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CustomSaveCallback('./unique_checkpoints', training_args.per_device_train_batch_size, tokenizer)]  # Pass the tokenizer here\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: Evaluate the model after training is complete\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "import datetime\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/standard_model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/standard_tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Standard with Larger Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.7244259119033813, 'eval_accuracy': 0.8407960199004975, 'eval_precision': 0.854400236988424, 'eval_recall': 0.8154216202361941, 'eval_f1': 0.8307566100839275, 'eval_f1_label_0': 0.8584905660377359, 'eval_f1_label_1': 0.8076923076923077, 'eval_f1_label_2': 0.8260869565217391, 'eval_runtime': 1.8258, 'eval_samples_per_second': 110.088, 'eval_steps_per_second': 7.12}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='354' max='354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [354/354 02:22, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Label 0</th>\n",
       "      <th>F1 Label 1</th>\n",
       "      <th>F1 Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.746253</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.848554</td>\n",
       "      <td>0.824284</td>\n",
       "      <td>0.832476</td>\n",
       "      <td>0.859903</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.845070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.853981</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.845436</td>\n",
       "      <td>0.815422</td>\n",
       "      <td>0.827687</td>\n",
       "      <td>0.858491</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.832117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.882762</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.850442</td>\n",
       "      <td>0.817995</td>\n",
       "      <td>0.828020</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.827586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.956428</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.853388</td>\n",
       "      <td>0.817327</td>\n",
       "      <td>0.830544</td>\n",
       "      <td>0.861244</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.822695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.945560</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.818566</td>\n",
       "      <td>0.830193</td>\n",
       "      <td>0.872038</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.039540</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.857276</td>\n",
       "      <td>0.820472</td>\n",
       "      <td>0.834310</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-1.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-2.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-3.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-4.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-5.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-6.0_batch-16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define training arguments and initialize the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CustomSaveCallback('./unique_checkpoints', training_args.per_device_train_batch_size, tokenizer)]  # Pass the tokenizer here\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: Evaluate the model after training is complete\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Final Model Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in directory: ./models/model_epochs-6_batch-16_2024-06-05_14-04-18\n",
      "Tokenizer saved in directory: ./models/tokenizer_epochs-6_batch-16_2024-06-05_14-04-18\n",
      "Evaluation results: {'eval_loss': 0.5747907757759094, 'eval_accuracy': 0.9003831417624522, 'eval_precision': 0.7691249388923808, 'eval_recall': 0.8673491673491673, 'eval_f1': 0.8080112044817928, 'eval_f1_label_0': 0.9411764705882353, 'eval_f1_label_1': 0.64, 'eval_f1_label_2': 0.8428571428571429, 'eval_runtime': 2.1672, 'eval_samples_per_second': 120.432, 'eval_steps_per_second': 7.844, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compute the confusion matrix\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(\u001b[43mtrue_labels\u001b[49m, predictions, labels\u001b[38;5;241m=\u001b[39munique_labels)\n\u001b[1;32m      6\u001b[0m display_labels \u001b[38;5;241m=\u001b[39m [reversed_label_map[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m unique_labels]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display the confusion matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions, labels=unique_labels)\n",
    "display_labels = [reversed_label_map[label] for label in unique_labels]\n",
    "\n",
    "# Display the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "disp.plot(values_format='d', cmap='Blues', ax=ax)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
