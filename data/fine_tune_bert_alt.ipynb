{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in /home/mwise/.local/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (2024.2.0)\n",
      "Requirement already satisfied: packaging in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: pandas in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (2.2.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (0.22.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: multiprocess in /home/mwise/.local/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/mwise/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: filelock in /home/mwise/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mwise/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/mwise/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/mwise/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mwise/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mwise/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mwise/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mwise/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mwise/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mwise/.local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mwise/.local/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mwise/.local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/mwise/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/mwise/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/mwise/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mwise/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mwise/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/mwise/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                context  label\n",
      "0     Speaker 1: A moderator, first of all. Howie, y...      2\n",
      "1     Speaker 1: A moderator, first of all. Howie, y...      2\n",
      "2     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "3     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "4     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "...                                                 ...    ...\n",
      "1332  Speaker 1: We were looking at some other thing...      0\n",
      "1333  Speaker 1: I cannot speculate on anything past...      0\n",
      "1334  Speaker 1: Yes. Look, as I said before, I thin...      2\n",
      "1335  Speaker 1: Yes. Look, as I said before, I thin...      0\n",
      "1336  Speaker 1: Yes. Look, as I said before, I thin...      2\n",
      "\n",
      "[1337 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('human_annotated_dataset.json', 'r') as f:\n",
    "    json_data = f.read()\n",
    "\n",
    "# Convert JSON to DataFrame\n",
    "data = json.loads(json_data)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Map labels to numerical values\n",
    "label_map = {'hedge': 2, 'authority': 1, 'none': 0}\n",
    "\n",
    "# Function to create separate rows for each marker\n",
    "def create_marker_rows(row):\n",
    "    statement = row['statement']\n",
    "    matched_terms = row['matched_terms']\n",
    "    rows = []\n",
    "    \n",
    "    for term, details in matched_terms.items():\n",
    "        term_upper = term.upper()\n",
    "        label = details['correct']\n",
    "        \n",
    "        # Add start and end markers\n",
    "        context = statement.replace(f'<{term_upper}>', f'[START] {term} [END]')\n",
    "        context = context.replace('<', '').replace('>', '')\n",
    "        if row['previous_statement'] == \"None\":\n",
    "            broad_context = \"Speaker 1: \" + context\n",
    "        else:\n",
    "            broad_context = \"Speaker 1: \" + row['previous_statement'] + \" Speaker 2: \" + context\n",
    "        rows.append({\n",
    "            'transcript_id': row['transcript_id'],\n",
    "            'statement_id': row['statement_id'],\n",
    "            'context': broad_context,\n",
    "            'label': label_map[label]\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# Create a new DataFrame with separate rows for each marker\n",
    "new_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    new_rows.extend(create_marker_rows(row))\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(new_df[['context', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in Random Sampling of \"Non-PPRMs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "                                                context  label\n",
      "0     Speaker 1: A moderator, first of all. Howie, y...      2\n",
      "1     Speaker 1: A moderator, first of all. Howie, y...      2\n",
      "2     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "3     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "4     Speaker 1: A moderator, first of all. Howie, y...      0\n",
      "...                                                 ...    ...\n",
      "1523  Speaker 1: The end is etched in stone. Exactly...      0\n",
      "1524  Speaker 1: Dylann Roof is not an immigrant. Sp...      0\n",
      "1525  Speaker 1: Dylann Roof is not an immigrant. Sp...      0\n",
      "1526  Speaker 1: The White House is reiterating that...      0\n",
      "1527  Speaker 1: The White House is reiterating that...      0\n",
      "\n",
      "[1528 rows x 2 columns]\n",
      "Speaker 1: A moderator, first of all. Howie, you know, some people have compared her to Nurse Ratched. Really what you have here is a junior high school sort of history teacher who doesn't really know the lesson plan and is sort of faking it by being didactic and over the top and assertive. She -- you know, there was a point where Mitt Romney answered a question and she said -- it was on taxes. And she said, \"OK, a little snappier, gentlemen.\" And you expected her to clap her hands twice like a nanny, or maybe a riding crop would have been a little bit better. She was so -- listen, you know, she had such a tin ear for the television medium. You thought they wouldn't bring her back the next day for the Democrats, but they did. Speaker 2: You KNOW, it's funny. When TV anchors MODERATE these things, whether it's Russert or Blitzer or Chris Matthews, people SOMETIMES say they're too aggressive, they turn it into a talk show, but MAYBE there's something to having somebody who knows something ABOUT television? WELL, she was as aggressive, just in a very\n",
      "Total Samples: 1528\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "# Load the non-matching utterances dataset\n",
    "with open('non_matching_utterances_sample.json', 'r') as f:\n",
    "    non_matching_data = json.load(f)\n",
    "print(len(non_matching_data))\n",
    "non_matching_df = pd.DataFrame(non_matching_data)\n",
    "\n",
    "# Adjust the function to ensure it correctly handles word selection\n",
    "def augment_non_matching_rows(row):\n",
    "    rows = []\n",
    "    statement = row['statement']\n",
    "    words = statement.split()\n",
    "    random_sample_num = min(2, len(words))\n",
    "    random_indices = random.sample(range(len(words)), random_sample_num)\n",
    "    for i in range(random_sample_num):\n",
    "        idx = random_indices[i]\n",
    "        statement = words.copy()\n",
    "        statement[idx] = '[START] ' + statement[idx] + ' [END]'\n",
    "        context = ' '.join(statement)\n",
    "        if row['previous_statement'] == \"None\":\n",
    "            broad_context = \"Speaker 1: \" + context\n",
    "        else:\n",
    "            broad_context = \"Speaker 1: \" + row['previous_statement'] + \" Speaker 2: \" + context\n",
    "        rows.append({\n",
    "            'transcript_id': row['transcript_id'],\n",
    "            'statement_id': str(uuid.uuid4()),\n",
    "            'context': broad_context,\n",
    "            'label': label_map['none']\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# Create a new DataFrame with separate rows for each marker\n",
    "non_match_rows = []\n",
    "for _, row in non_matching_df.iterrows():\n",
    "    non_match_rows.extend(augment_non_matching_rows(row))\n",
    "\n",
    "other_df = pd.DataFrame(non_match_rows)\n",
    "\n",
    "# Combine the existing DataFrame with the augmented data\n",
    "# Ensure new_df is already defined and contains initial data\n",
    "augmented_df = pd.concat([new_df, other_df], ignore_index=True)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(augmented_df[['context', 'label']])\n",
    "print(augmented_df.head()[\"context\"][3])\n",
    "# for k, v in augmented_df[:1].items():\n",
    "#     print(v)\n",
    "\n",
    "print(\"Total Samples:\", len(augmented_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwise/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and Train the Model\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Adjust num_labels to 3\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device='cpu'\n",
    "model.to(device)\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "class CustomSaveCallback(TrainerCallback):\n",
    "    \"A custom callback that saves the model at the end of each epoch with a unique name.\"\n",
    "    def __init__(self, save_path, batch_size, tokenizer):\n",
    "        self.save_path = save_path\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer  # Save the tokenizer as an instance variable\n",
    "\n",
    "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # Format the directory name with the epoch number and batch size\n",
    "        epoch_dir = f\"{self.save_path}/model_checkpoint_epoch-{state.epoch}_batch-{self.batch_size}\"\n",
    "        if not os.path.exists(epoch_dir):\n",
    "            os.makedirs(epoch_dir)\n",
    "        # Save the model and tokenizer in this directory\n",
    "        kwargs['model'].save_pretrained(epoch_dir)\n",
    "        self.tokenizer.save_pretrained(epoch_dir)  # Use the instance variable\n",
    "        print(f\"Saved model and tokenizer to {epoch_dir}\")\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    # Compute F1 for each label individually\n",
    "    f1_per_class = f1_metric.compute(predictions=predictions, references=labels, average=None)\n",
    "    label_f1_scores = f1_per_class['f1']\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall'],\n",
    "        'f1': f1['f1'],\n",
    "        'f1_label_0': label_f1_scores[0],\n",
    "        'f1_label_1': label_f1_scores[1],\n",
    "        'f1_label_2': label_f1_scores[2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on non-Augmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Train/Eval/Test, Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating new datasets\n",
      "Length of files: 935 201 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 935/935 [00:02<00:00, 333.42 examples/s]\n",
      "Map: 100%|██████████| 201/201 [00:00<00:00, 319.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "regen_data = True\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the logging level to ERROR to reduce output clutter\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['context'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "# File paths for saved datasets\n",
    "train_file = 'train_dataset_nonaug.pkl'\n",
    "eval_file = 'eval_dataset_nonaug.pkl'\n",
    "test_file = 'test_dataset_nonaug.pkl'\n",
    "\n",
    "# Function to save a dataset\n",
    "def save_dataset(data, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "# Function to load a dataset\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "# Check if regeneration of data is needed\n",
    "if regen_data:\n",
    "    print(\"Generating new datasets\")\n",
    "    # Split the data into training and evaluation sets\n",
    "    train_eval_df, test_df = train_test_split(new_df, test_size=0.15, random_state=42, stratify=new_df['label'])\n",
    "    # Ensure to use train_eval_df for further splitting and stratification\n",
    "    train_df, eval_df = train_test_split(train_eval_df, test_size=0.1764705882352941, random_state=42, stratify=train_eval_df['label'])\n",
    "    save_dataset(train_df, train_file)\n",
    "    save_dataset(eval_df, eval_file)\n",
    "    save_dataset(test_df, test_file)\n",
    "    print(\"Length of files:\", len(train_df), len(eval_df), len(test_df))\n",
    "elif os.path.exists(train_file) and os.path.exists(eval_file) and os.path.exists(test_file):\n",
    "    # Load the datasets\n",
    "    train_df = load_dataset(train_file)\n",
    "    eval_df = load_dataset(eval_file)\n",
    "    test_df = load_dataset(test_file)\n",
    "    print(\"Length of files:\", len(train_df), len(eval_df), len(test_df))\n",
    "else:\n",
    "    # Split the data into training and evaluation sets\n",
    "    train_eval_df, test_df = train_test_split(augmented_df, test_size=0.15, random_state=42, stratify = augmented_df['label'])\n",
    "    train_df, eval_df = train_test_split(train_eval_df, test_size=0.1764705882352941, random_state=42, stratify = train_eval_df['label'])\n",
    "    save_dataset(train_df, train_file)\n",
    "    save_dataset(eval_df, eval_file)\n",
    "    save_dataset(test_df, test_file)\n",
    "    print(\"Length of files:\", len(train_df), len(eval_df), len(test_df))\n",
    "    \n",
    "from datasets import Dataset\n",
    "# Convert DataFrame to Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_tokenized = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\", config=\"multiclass\")\n",
    "recall_metric = evaluate.load(\"recall\", config=\"multiclass\")\n",
    "f1_metric = evaluate.load(\"f1\", config=\"multiclass\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: label\n",
      "0    490\n",
      "1    136\n",
      "2    309\n",
      "Name: count, dtype: int64\n",
      "Class Weights: tensor([0.6361, 2.2917, 1.0086], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Example class counts: You might need to calculate or provide actual counts\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "print(\"Class counts:\", class_counts)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "# Print class weights to verify\n",
    "print(\"Class Weights:\", class_weights_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Weighted Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model, class_weights):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = model  # this should be an instance of a Hugging Face PreTrainedModel\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        if labels is not None:\n",
    "            self.class_weights = self.class_weights.to(self.model.device)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            return (loss, logits)\n",
    "        return logits\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\"Save the contained PreTrainedModel to a directory.\"\"\"\n",
    "        self.model.save_pretrained(save_directory)\n",
    "\n",
    "    def from_pretrained(self, load_directory):\n",
    "        \"\"\"Load the contained PreTrainedModel from a directory.\"\"\"\n",
    "        self.model = self.model.from_pretrained(load_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Weighted Loss Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='702' max='702' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [702/702 12:44, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Label 0</th>\n",
       "      <th>F1 Label 1</th>\n",
       "      <th>F1 Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.098872</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.236879</td>\n",
       "      <td>0.360009</td>\n",
       "      <td>0.262821</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.066049</td>\n",
       "      <td>0.482587</td>\n",
       "      <td>0.432145</td>\n",
       "      <td>0.433361</td>\n",
       "      <td>0.374875</td>\n",
       "      <td>0.658120</td>\n",
       "      <td>0.314607</td>\n",
       "      <td>0.151899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.534482</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.819870</td>\n",
       "      <td>0.814909</td>\n",
       "      <td>0.817295</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.812030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.697171</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.810222</td>\n",
       "      <td>0.832942</td>\n",
       "      <td>0.818470</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.811189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.735050</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.884474</td>\n",
       "      <td>0.845938</td>\n",
       "      <td>0.862489</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.835821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.655482</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.894430</td>\n",
       "      <td>0.869439</td>\n",
       "      <td>0.879733</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.855072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-1.0_batch-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwise/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-2.0_batch-8\n",
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-3.0_batch-8\n",
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-4.0_batch-8\n",
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-5.0_batch-8\n",
      "Saved model and tokenizer to ./weighted_checkpoints/model_checkpoint_epoch-6.0_batch-8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6554823517799377, 'eval_accuracy': 0.8805970149253731, 'eval_precision': 0.8944301994301993, 'eval_recall': 0.8694393512220447, 'eval_f1': 0.8797331492983668, 'eval_f1_label_0': 0.8952380952380953, 'eval_f1_label_1': 0.8888888888888888, 'eval_f1_label_2': 0.855072463768116, 'eval_runtime': 11.9856, 'eval_samples_per_second': 16.77, 'eval_steps_per_second': 2.169, 'epoch': 6.0}\n",
      "Model saved in directory: ./models/weighted_model_epochs-6_batch-8_2024-06-06_15-17-33\n",
      "Tokenizer saved in directory: ./models/weighted_tokenizer_epochs-6_batch-8_2024-06-06_15-17-33\n",
      "Evaluation results: {'eval_loss': 0.6554823517799377, 'eval_accuracy': 0.8805970149253731, 'eval_precision': 0.8944301994301993, 'eval_recall': 0.8694393512220447, 'eval_f1': 0.8797331492983668, 'eval_f1_label_0': 0.8952380952380953, 'eval_f1_label_1': 0.8888888888888888, 'eval_f1_label_2': 0.855072463768116, 'eval_runtime': 11.9856, 'eval_samples_per_second': 16.77, 'eval_steps_per_second': 2.169, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Initialize the model and ensure it and its weights are on the correct device\n",
    "model_with_loss = CustomModel(original_model.to(device), class_weights_tensor.to(device))\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the custom model\n",
    "trainer = Trainer(\n",
    "    model=model_with_loss,  # Ensure this is your custom model accepting weights\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,  # Ensure datasets are correctly tokenized\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,  # Custom metrics function if needed\n",
    "    callbacks=[CustomSaveCallback('./weighted_checkpoints', training_args.per_device_train_batch_size, tokenizer)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "import datetime\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/weighted_model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/weighted_tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.1028903722763062, 'eval_accuracy': 0.3681592039800995, 'eval_precision': 0.34761904761904755, 'eval_recall': 0.35113989603254403, 'eval_f1': 0.29903267278450146, 'eval_f1_label_0': 0.3013698630136986, 'eval_f1_label_1': 0.12, 'eval_f1_label_2': 0.47572815533980584, 'eval_runtime': 8.1016, 'eval_samples_per_second': 24.81, 'eval_steps_per_second': 3.209}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='702' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/702 00:02 < 09:53, 1.18 it/s, Epoch 0.03/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_results)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2208\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define training arguments and initialize the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CustomSaveCallback('./unique_checkpoints', training_args.per_device_train_batch_size, tokenizer)]  # Pass the tokenizer here\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: Evaluate the model after training is complete\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "import datetime\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/standard_model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/standard_tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Standard with Larger Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.7244259119033813, 'eval_accuracy': 0.8407960199004975, 'eval_precision': 0.854400236988424, 'eval_recall': 0.8154216202361941, 'eval_f1': 0.8307566100839275, 'eval_f1_label_0': 0.8584905660377359, 'eval_f1_label_1': 0.8076923076923077, 'eval_f1_label_2': 0.8260869565217391, 'eval_runtime': 1.8258, 'eval_samples_per_second': 110.088, 'eval_steps_per_second': 7.12}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='354' max='354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [354/354 02:22, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Label 0</th>\n",
       "      <th>F1 Label 1</th>\n",
       "      <th>F1 Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.746253</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.848554</td>\n",
       "      <td>0.824284</td>\n",
       "      <td>0.832476</td>\n",
       "      <td>0.859903</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.845070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.853981</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.845436</td>\n",
       "      <td>0.815422</td>\n",
       "      <td>0.827687</td>\n",
       "      <td>0.858491</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.832117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.882762</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.850442</td>\n",
       "      <td>0.817995</td>\n",
       "      <td>0.828020</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.827586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.956428</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.853388</td>\n",
       "      <td>0.817327</td>\n",
       "      <td>0.830544</td>\n",
       "      <td>0.861244</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.822695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.945560</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.818566</td>\n",
       "      <td>0.830193</td>\n",
       "      <td>0.872038</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.039540</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.857276</td>\n",
       "      <td>0.820472</td>\n",
       "      <td>0.834310</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-1.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-2.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-3.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-4.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-5.0_batch-16\n",
      "Saved model and tokenizer to ./unique_checkpoints/model_checkpoint_epoch-6.0_batch-16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define training arguments and initialize the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CustomSaveCallback('./unique_checkpoints', training_args.per_device_train_batch_size, tokenizer)]  # Pass the tokenizer here\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: Evaluate the model after training is complete\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Final Model Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in directory: ./models/model_epochs-6_batch-16_2024-06-05_14-04-18\n",
      "Tokenizer saved in directory: ./models/tokenizer_epochs-6_batch-16_2024-06-05_14-04-18\n",
      "Evaluation results: {'eval_loss': 0.5747907757759094, 'eval_accuracy': 0.9003831417624522, 'eval_precision': 0.7691249388923808, 'eval_recall': 0.8673491673491673, 'eval_f1': 0.8080112044817928, 'eval_f1_label_0': 0.9411764705882353, 'eval_f1_label_1': 0.64, 'eval_f1_label_2': 0.8428571428571429, 'eval_runtime': 2.1672, 'eval_samples_per_second': 120.432, 'eval_steps_per_second': 7.844, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# Get current datetime to use as a unique identifier\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define model parameters to include in the filename\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Define the directory using the timestamp and model parameters\n",
    "model_dir = f'./models/model_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "tokenizer_dir = f'./models/tokenizer_epochs-{num_epochs}_batch-{batch_size}_{current_time}'\n",
    "\n",
    "# Save the model and tokenizer with detailed names\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"Model saved in directory: {model_dir}\")\n",
    "print(f\"Tokenizer saved in directory: {tokenizer_dir}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compute the confusion matrix\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(\u001b[43mtrue_labels\u001b[49m, predictions, labels\u001b[38;5;241m=\u001b[39munique_labels)\n\u001b[1;32m      6\u001b[0m display_labels \u001b[38;5;241m=\u001b[39m [reversed_label_map[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m unique_labels]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display the confusion matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions, labels=unique_labels)\n",
    "display_labels = [reversed_label_map[label] for label in unique_labels]\n",
    "\n",
    "# Display the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "disp.plot(values_format='d', cmap='Blues', ax=ax)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
